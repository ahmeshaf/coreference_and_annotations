{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b9f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/coreference_and_annotations\n",
      "/s/chopin/d/proj/ramfis-aida/coreference_and_annotations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "os.chdir('/s/chopin/d/proj/ramfis-aida/coreference_and_annotations')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1cab2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LongformerModel \n",
    "# from longformer.longformer import Longformer, LongformerConfig\n",
    "# from longformer.sliding_chunks import pad_to_window_size\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import *\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import copy\n",
    "import glob\n",
    "import os.path\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch \n",
    "import pyhocon\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3311d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from parsing.parse_ldc import extract_mentions\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from evaluations.eval import *\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590410ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference\n",
      "/s/chopin/d/proj/ramfis-aida/coreference_and_annotations\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c49d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mentions_csv_path = '/s/chopin/d/proj/ramfis-aida/New_codescripts_embeddings/mentions_csv/mentions.csv'\n",
    "mentions_csv_path = '/s/chopin/d/proj/ramfis-aida/New_codescripts_embeddings/mentions_csv_ent_ldc/ent_map.csv'\n",
    "#mentions_csv_path = '/s/chopin/d/proj/ramfis-aida/CDLM-main_old/gaia_mentions_info/gaia_mentions_info/eve_map.csv'\n",
    "\n",
    "#source_dir = '/s/chopin/d/proj/ramfis-aida/New_codescripts_embeddings'\n",
    "source_dir = '/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/LDCsource'\n",
    "ann_dir = '/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/LDCann'\n",
    "\n",
    "output_folder = '/s/chopin/d/proj/ramfis-aida/New_codescripts_embeddings'\n",
    "working_folder = output_folder + '/WORKING_LDC_PHASE3_cosim/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "b299aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_pair_similarity_lemma(mention_pairs, mention_map, relations, working_folder):\n",
    "    \"\"\"\n",
    "    Generate the similarities for mention pairs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mention_pairs: list\n",
    "    mention_map: dict\n",
    "    relations: list\n",
    "        The list of relations represented as a triple: (head, label, tail)\n",
    "    working_folder: str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "\n",
    "    # generate similarity using the mention text\n",
    "    for pair in mention_pairs:\n",
    "        men1, men2 = pair\n",
    "        men_map1 = mention_map[men1]\n",
    "        men_map2 = mention_map[men2]\n",
    "        men_text1 = men_map1['mention_text']\n",
    "        men_text2 = men_map2['mention_text']\n",
    "        similarities.append(int(men_text1 == men_text2))\n",
    "        #print(similarities)\n",
    "        #print(len(similarities))\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def get_mention_pair_cosinesimilarity(mention_pairs, mention_map,evt_vec_map, relations, working_folder):\n",
    "    \"\"\"\n",
    "    Generate the cosine similarities for mention pairs using the CDLM model \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mention_pairs: list\n",
    "    mention_map: dict\n",
    "    evt_vec_map : dict with mention IDs as keys and CDLM Embeddings as values\n",
    "    relations: list\n",
    "        The list of relations represented as a triple: (head, label, tail)\n",
    "    working_folder: str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    sims_list = []\n",
    "    cos_s = torch.nn.CosineSimilarity()\n",
    "      # generate similarity using the mention text\n",
    "    for pair in mention_pairs:\n",
    "        men1, men2 = pair\n",
    "        men_vec1 = torch.tensor(evt_vec_map[men1].reshape(1,-1)) \n",
    "        men_vec2 = torch.tensor(evt_vec_map[men2].reshape(1,-1))\n",
    "        sims = cos_s(men_vec1,men_vec2).data.cpu().numpy()\n",
    "        #sims[sims>0.98757007] = 1  #mean for cdlm model \n",
    "        #sims[sims<=0.98757007] =0\n",
    "        #sims[sims>0.95] = 1\n",
    "        #sims[sims<=0.95] =0\n",
    "        #sims[sims>0.47] = 1  # mean for coref bert model \n",
    "        #sims[sims<=0.47] =0\n",
    "#         sims[sims>0.39636973] = 1  # mean for bert large  model \n",
    "#         sims[sims<=0.39636973] =0\n",
    "        \n",
    "#         sims[sims>0.6] = 1  #  \n",
    "#         sims[sims<=0.6] =0\n",
    "        \n",
    "        sims[sims>0.7] = 1  #  \n",
    "        sims[sims<=0.7] =0\n",
    "        sims_list.append([pair, sims])\n",
    "        similarities.append(sims)\n",
    "\n",
    "    return similarities, sims_list\n",
    "\n",
    "def cluster_cc(affinity_matrix, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Find connected components using the affinity matrix and threshold -> adjacency matrix\n",
    "    Parameters\n",
    "    ----------\n",
    "    affinity_matrix: np.array\n",
    "    threshold: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list, np.array\n",
    "    \"\"\"\n",
    "    adjacency_matrix = csr_matrix(affinity_matrix > threshold)\n",
    "    clusters, labels = connected_components(adjacency_matrix, return_labels=True, directed=False)\n",
    "    return clusters, labels\n",
    "\n",
    "\n",
    "def run_coreference(ann_dir, source_dir, working_folder, men_type='evt'):\n",
    "    \"\"\"\n",
    "    Run the coreference resolution pipeline on LDC annotations in the following steps:\n",
    "        1) Read and parse the annotations + source documents\n",
    "        2) Generate pairs of mentions from the same topic\n",
    "        3) Generate similarities for the pairs and create a similarity matrix\n",
    "        4) Agglomeratively cluster using the similarity matrix\n",
    "        5) Evaluate the clusters against the gold standard clusters (BCUB, MUC, CEAF, CONNL)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ann_dir: str\n",
    "    source_dir: str\n",
    "    working_folder: str\n",
    "    men_type: str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # create working folder\n",
    "    if not os.path.exists(working_folder):\n",
    "        os.makedirs(working_folder)\n",
    "\n",
    "    # extract the mention maps\n",
    "    eve_mention_map, ent_mention_map, relations, doc_sent_map = extract_mentions(ann_dir, source_dir, working_folder)\n",
    "\n",
    "    # which coreference mention map\n",
    "    if men_type == 'evt':\n",
    "        curr_mention_map = eve_mention_map\n",
    "    else:\n",
    "        curr_mention_map = ent_mention_map\n",
    "\n",
    "    # create a single dict for all mentions\n",
    "    all_mention_map = {**eve_mention_map, **ent_mention_map}\n",
    "\n",
    "    # sort event mentions and make men to ind map\n",
    "    curr_mentions = sorted(list(curr_mention_map.keys()), key=lambda x: curr_mention_map[x]['m_id'])\n",
    "    curr_men_to_ind = {eve: i for i, eve in enumerate(curr_mentions)}\n",
    "\n",
    "    # generate gold clusters key file\n",
    "    curr_gold_cluster_map = [(men, all_mention_map[men]['gold_cluster']) for men in curr_mentions]\n",
    "    gold_key_file = working_folder + f'/{men_type}_gold.keyfile'\n",
    "    generate_key_file(curr_gold_cluster_map, men_type, working_folder, gold_key_file)\n",
    "\n",
    "    # group mentions by topic\n",
    "    topic_mention_dict = defaultdict(list)\n",
    "    for men_id, coref_map in curr_mention_map.items():\n",
    "        topic = coref_map['topic']\n",
    "        topic_mention_dict[topic].append(men_id)\n",
    "\n",
    "    # generate mention-pairs\n",
    "    mention_pairs = []\n",
    "    for mentions in topic_mention_dict.values():\n",
    "        list_mentions = list(mentions)\n",
    "        for i in range(len(list_mentions)):\n",
    "            for j in range(i+1):\n",
    "                if i != j:\n",
    "                    mention_pairs.append((list_mentions[i], list_mentions[j]))\n",
    "\n",
    "    # get the similarities of the mention-pairs\n",
    "    similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "    # get indices\n",
    "    mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "    row, col = zip(*mention_ind_pairs)\n",
    "\n",
    "    # create similarity matrix from the similarities\n",
    "    n = len(curr_mentions)\n",
    "    similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "    # clustering algorithm and mention cluster map\n",
    "    clusters, labels = connected_components(similarity_matrix)\n",
    "    system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "    # generate system key file\n",
    "    system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "    generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "    # evaluate\n",
    "    generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "379c6a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/parsing/parse_ldc.py:114: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 114 of the file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/parsing/parse_ldc.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  file_bs = bs(ltf_xml, from_encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "men_type = 'evt'\n",
    "\n",
    "eve_mention_map, ent_mention_map, relations, doc_sent_map  = extract_mentions(ann_dir, source_dir, working_folder)\n",
    "\n",
    "# # which coreference mention map\n",
    "# if men_type == 'evt':\n",
    "#     curr_mention_map = eve_mention_map\n",
    "# else:\n",
    "#     curr_mention_map = ent_mention_map\n",
    "\n",
    "# # create a single dict for all mentions\n",
    "# all_mention_map = {**eve_mention_map, **ent_mention_map}\n",
    "\n",
    "# # sort event mentions and make men to ind map\n",
    "# curr_mentions = sorted(list(curr_mention_map.keys()), key=lambda x: curr_mention_map[x]['m_id'])\n",
    "# curr_men_to_ind = {eve: i for i, eve in enumerate(curr_mentions)}\n",
    "\n",
    "# # generate gold clusters key file\n",
    "# curr_gold_cluster_map = [(men, all_mention_map[men]['gold_cluster']) for men in curr_mentions]\n",
    "# gold_key_file = working_folder + f'/{men_type}_gold.keyfile'\n",
    "# generate_key_file(curr_gold_cluster_map, men_type, working_folder, gold_key_file)\n",
    "\n",
    "# # group mentions by topic\n",
    "# topic_mention_dict = defaultdict(list)\n",
    "# for men_id, coref_map in curr_mention_map.items():\n",
    "#     topic = coref_map['topic']\n",
    "#     topic_mention_dict[topic].append(men_id)\n",
    "\n",
    "# # generate mention-pairs\n",
    "# mention_pairs = []\n",
    "# for mentions in topic_mention_dict.values():\n",
    "#     list_mentions = list(mentions)\n",
    "#     for i in range(len(list_mentions)):\n",
    "#         for j in range(i+1):\n",
    "#             if i != j:\n",
    "#                 mention_pairs.append((list_mentions[i], list_mentions[j]))\n",
    "\n",
    "# # get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e33e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "eve_mention_map_file = working_folder + '/evt_mention_map_bertdoc.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "000afb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_map_evt_bertdoc = pickle.load(open(eve_mention_map_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5f4706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m_id': 704,\n",
       " 'mention_id': 'VMIC0015MVH.001906',\n",
       " 'doc_id': 'HC0008OSY',\n",
       " 'start_char': 3630,\n",
       " 'end_char': 3638,\n",
       " 'gold_cluster': 'NILV03220',\n",
       " 'topic': 'E101',\n",
       " 'lang_id': 'eng',\n",
       " 'mention_text': 'a warning',\n",
       " 'sentence_start_char': 3607,\n",
       " 'sentence': 'The murder is meant as a warning for all those who dare to oppose Kiev government, which can do anything against the opponents, Tsarev believes.',\n",
       " 'bert_sentence': 'The murder is meant as  <m> a warning </m>  for all those who dare to oppose Kiev government, which can do anything against the opponents, Tsarev believes.',\n",
       " 'bert_doc': 'Series of ‘bizarre suicides’ &amp;amp; murders: Former Ukrainian MP shot dead in Kiev Oleg Kalashnikov (image by http://glavpost.com) \\u200bA former Ukrainian MP and active anti-Maidan activist, Oleg Kalashnikov, has been killed in his flat in Kiev. His killing is the latest in a series of odd deaths plaguing former government officials and ex-President Yanukovichs party members. The 52-year-old was found dead at his residence in Kiev on Wednesday evening. His death was caused by a gunshot, the Interior Ministry said in a statement announcing a police inquiry. Ukraines criminal investigation chief Vasily Paskal, took the investigation under personal control and promised to share motives and the preliminary results of the probe with reporters as soon as they become available. The investigation is focused on five possible motives for the crime, according to Interior Ministers senior adviser, Anton Gerashchenko. So far the investigation considers the primary possible motive behind the killing to be Kalashnikovs political activity linked with his participation in the organization and financing of counter-revolutionary events in Ukraine. Gerashchenko emphasized that Kalashnikov had knowledge of the anti-Maidan movement that resisted the coup last year and continues to challenge new authorities in Kiev. Without any doubt the deceased knew a lot about who and in what way financed anti-Maidan, which cost Yanukovich and his camarilla several million hryvnias per day. He takes these secrets with him to the grave, Gerashchenko said, also listing some other leads on his Facebook page. Business debts, personal enmity, burglary attempt and other versions of murder are listed among other possible motives. Убит экс-депутат Партии регионов Олег Калашников http://t.co/jj0hfxAQoopic.twitter.com/PDJ0ZZeLSl Новости Донбасса (@novostidnua) April 15, 2015 Ukrainian media reported that before the murder Kalashnikov received threats of physical violence for his political views, in particular for his drive to defend Ukrainians right to widely celebrate the 70th anniversary of WWII victory. In a letter addressed to his friend before the murder, Kalashnikov allegedly wrote that an open genocide on dissent, death threats and constant dirty insults have become the norm following his open call to honor the memory of heroes and victims of the Great Patriotic War. An acting Ukrainian MP and ex-spokesman for the extremist Right Sector group, Borislav Bereza, went further and alleged that Kalashnikov has been eliminated by his former employers, who were tying up loose ends, scared he could disclose details of their past activities. While part of the secret was taken to the grave, some information remained in electronic form, Bereza stated A series of bizarre suicides of ex-regionals [Members of the Party of Regions], and now the murder of Kalashnikov, raises questions to law enforcement authorities. I hope that Ukrainian society will get the answers, Bereza said. Meanwhile, Oleg Tsarev, parliamentary speaker of the self-proclaimed Novorossiya, agreed that Kalashnikovs murder is the latest link in a chain of mysterious deaths of former supporters of the Party of Regions. Of course, this is a political murder. In Ukraine, it is now extremely difficult to maintain your point of view, not to give up, and to publicly express it, Tsarev told Lifenews. Of course, this is a retaliatory murder of the sane. Организатор антимайданов Олег Калашников убит в среду вечером в Киеве. pic.twitter.com/fGAn5XIHH1 Спутник АТО (@SputnikATO) April 15, 2015 The murder is meant as  <m> a warning </m>  for all those who dare to oppose Kiev government, which can do anything against the opponents, Tsarev believes. In the past few months, at least eight former Ukrainian government officials died mysterious deaths, with most treated as suicides. On January 29, former chairman of Kharkov region government, Aleksey Kolesnik, was found hanged. On February 24, former Party of Regions member Stanislav Melnik died of a gunshot with his death treated as suicide. On February 25, several hours before his trial, the Mayor of Melitopol Sergey Valter was found hanged leaving no suicide note. The next day, February 26, deputy chief of Melitopol police, Aleksandr Bordyuga, who reportedly acted as Valters lawyer, was found dead in his garage. On February 26, a former MP and ex-chairman of Zaporozhye Regional State Administration was found dead with a gun wound to his neck. His death is being investigated as a suicide. On February 28, former member of the Party of Regions, Mikhail Chechetov, jumped from the window of his 17th floor apartment in Kiev, leaving a suicide note. On March 14, a 32-year-old prosecutor Sergey Melnichuk fell from a window of a 9th floor apartment in Odessa.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_map_evt_bertdoc['VMIC0015MVH.001906']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd15e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_map_path = working_folder + '/cdlm_evt_vector_map_events_LDCPhase3.pkl'\n",
    "#men_ids, bert_sentences = zip(*[(men_id, men['bert_sentence']) for men_id, men in eve_mention_map.items()])\n",
    "men_ids, bert_sentences = zip(*[(men_id, men['bert_doc']) for men_id, men in vector_map_evt_bertdoc.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ad1275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/vocab.txt\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/tokenizer.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/added_tokens.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/special_tokens_map.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/tokenizer_config.json\n",
      "loading configuration file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28998\n",
      "}\n",
      "\n",
      "loading weights file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert_coref = AutoTokenizer.from_pretrained('/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token')\n",
    "model_coref = AutoModel.from_pretrained('/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ac4a8c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/vocab.txt\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/added_tokens.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/special_tokens_map.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/tokenizer_config.json\n",
      "Adding <m> to the vocabulary\n",
      "Adding </m> to the vocabulary\n",
      "loading configuration file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30524\n",
      "}\n",
      "\n",
      "loading weights file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert_large = BertTokenizer.from_pretrained('/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large')\n",
    "model_bert_large = BertModel.from_pretrained(\"/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "cfb9ce75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28996, 28997)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_id = tokenizer_bert_coref.encode('<m>', add_special_tokens=False)[0]\n",
    "end_id = tokenizer_bert_coref.encode('</m>', add_special_tokens=False)[0]\n",
    "start_id,end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "46b4dc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large\",\n",
       "  \"architectures\": [\n",
       "    \"BertModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.18.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30524\n",
       "}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_coref.config \n",
    "model_bert_large.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "92513217",
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_model_path ='/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base'\n",
    "bert_large_path = \"/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large\"\n",
    "bert_large_tokenizer_path = '/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "476c0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.uniform_(m.bias)\n",
    "\n",
    "class FullCrossEncoder(nn.Module):\n",
    "    def __init__(self, config, is_training=True, long=False):\n",
    "        super(FullCrossEncoder, self).__init__()\n",
    "        #self.segment_size = config.segment_window * 2\n",
    "        #self.tokenizer = AutoTokenizer.from_pretrained('/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_large_tokenizer_path)\n",
    "        \n",
    "\n",
    "\n",
    "        #self.tokenizer.add_tokens(['<m>', '</m>'], special_tokens = True)\n",
    "        #self.tokenizer.add_tokens(['<g>'], special_tokens = True)\n",
    "        self.start_id = self.tokenizer.encode('<m>', add_special_tokens=False)[0]\n",
    "        self.end_id = self.tokenizer.encode('</m>', add_special_tokens=False)[0]\n",
    "\n",
    "        self.vals = [self.start_id, self.end_id]\n",
    "        self.long = long\n",
    "        if not is_training and config.pretrained_model:\n",
    "            self.model = AutoModel.from_pretrained(config.cdlm_path)\n",
    "        else:\n",
    "            #self.model = AutoModel.from_pretrained(coref_model_path)\n",
    "            self.model = BertModel.from_pretrained(bert_large_path)\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        if not self.long:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size*4, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        self.linear.apply(init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, arg1=None, arg2=None):\n",
    "        output, _ = self.model(input_ids, attention_mask=attention_mask)\n",
    "        if self.long:\n",
    "            arg1_vec = (output*arg1.unsqueeze(-1)).sum(1)\n",
    "            arg2_vec = (output*arg2.unsqueeze(-1)).sum(1)\n",
    "        cls_vector = output[:, 0, :]\n",
    "        if not self.long:\n",
    "            scores = self.linear(cls_vector)\n",
    "        else:\n",
    "            scores = self.linear(torch.cat([cls_vector, arg1_vec, arg2_vec, arg1_vec*arg2_vec],dim=1))\n",
    "        return scores\n",
    "\n",
    "    def generate_rep(self, input_ids, attention_mask=None, arg1=None,):\n",
    "        output, _ = self.model(input_ids, attention_mask=attention_mask)\n",
    "        arg1_vec = (output*arg1.unsqueeze(-1)).sum(1)\n",
    "        return arg1_vec\n",
    "\n",
    "\n",
    "class FullCrossEncoderSingle(FullCrossEncoder):\n",
    "    def __init__(self, config, is_training=True, long=False):\n",
    "        super(FullCrossEncoderSingle, self).__init__(config, is_training=is_training, long=long)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, arg1=None, arg2=None):\n",
    "        output = self.model(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "         \n",
    "        arg1_vec = (output * arg1.unsqueeze(-1)).sum(1)\n",
    "        return arg1_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f5132428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/vocab.txt\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/tokenizer.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/added_tokens.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/special_tokens_map.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token/tokenizer_config.json\n",
      "loading configuration file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28998\n",
      "}\n",
      "\n",
      "loading weights file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "cross_encoder = FullCrossEncoderSingle(model_coref.config , long=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "74874334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "Allocated: 1.9 GB\n",
      "Cached:    2.8 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "da292ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "cross_encoder = cross_encoder.to(device)\n",
    "cross_encoder.model = AutoModel.from_pretrained(coref_model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7c8068ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28998\n",
      "}\n",
      "\n",
      "loading weights file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "aba8633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullCrossEncoderSingle(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28998, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_coref_model = torch.nn.DataParallel(cross_encoder)\n",
    "parallel_coref_model.module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "736677bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/coref_bert_base/coref_token', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<m>', '</m>']})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_coref_model.eval()\n",
    "parallel_coref_model.module.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abd5a1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28997"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bert_sentences =bert_sentences[0:20]\n",
    "parallel_coref_model.module.vals[0]\n",
    "parallel_coref_model.module.vals[1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "17a1dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([847, 2])\n",
      "torch.Size([847, 12006])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([134, 136]),\n",
       " array([69, 71]),\n",
       " array([162, 164]),\n",
       " array([491, 493]),\n",
       " array([663, 666]),\n",
       " array([362, 366]),\n",
       " array([747, 750]),\n",
       " array([748, 750]),\n",
       " array([921, 924]),\n",
       " array([963, 965]),\n",
       " array([ 999, 1001]),\n",
       " array([ 999, 1001]),\n",
       " array([1028, 1030]),\n",
       " array([1025, 1027]),\n",
       " array([1058, 1060]),\n",
       " array([1058, 1060]),\n",
       " array([1103, 1105]),\n",
       " array([1135, 1137]),\n",
       " array([1135, 1137]),\n",
       " array([1148, 1150]),\n",
       " array([1176, 1178]),\n",
       " array([1191, 1194]),\n",
       " array([1213, 1215]),\n",
       " array([1213, 1218]),\n",
       " array([243, 248]),\n",
       " array([663, 666]),\n",
       " array([560, 563]),\n",
       " array([1041, 1043]),\n",
       " array([1061, 1064]),\n",
       " array([1192, 1195]),\n",
       " array([301, 303]),\n",
       " array([40, 43]),\n",
       " array([441, 443]),\n",
       " array([277, 279]),\n",
       " array([607, 610]),\n",
       " array([623, 626]),\n",
       " array([643, 646]),\n",
       " array([440, 442]),\n",
       " array([614, 617]),\n",
       " array([631, 633]),\n",
       " array([644, 646]),\n",
       " array([502, 505]),\n",
       " array([38, 40]),\n",
       " array([28, 31]),\n",
       " array([28, 31]),\n",
       " array([63, 66]),\n",
       " array([63, 66]),\n",
       " array([133, 136]),\n",
       " array([133, 136]),\n",
       " array([5, 7]),\n",
       " array([54, 56]),\n",
       " array([54, 56]),\n",
       " array([83, 85]),\n",
       " array([83, 85]),\n",
       " array([125, 127]),\n",
       " array([125, 127]),\n",
       " array([143, 146]),\n",
       " array([162, 164]),\n",
       " array([171, 173]),\n",
       " array([194, 196]),\n",
       " array([222, 224]),\n",
       " array([322, 324]),\n",
       " array([357, 359]),\n",
       " array([415, 420]),\n",
       " array([451, 453]),\n",
       " array([434, 437]),\n",
       " array([510, 512]),\n",
       " array([516, 518]),\n",
       " array([520, 522]),\n",
       " array([526, 528]),\n",
       " array([542, 544]),\n",
       " array([544, 546]),\n",
       " array([647, 649]),\n",
       " array([658, 660]),\n",
       " array([660, 665]),\n",
       " array([88, 90]),\n",
       " array([61, 63]),\n",
       " array([88, 90]),\n",
       " array([256, 258]),\n",
       " array([256, 258]),\n",
       " array([207, 209]),\n",
       " array([298, 300]),\n",
       " array([298, 300]),\n",
       " array([21, 23]),\n",
       " array([44, 46]),\n",
       " array([ 9, 11]),\n",
       " array([42, 44]),\n",
       " array([41, 43]),\n",
       " array([72, 74]),\n",
       " array([ 9, 11]),\n",
       " array([ 8, 10]),\n",
       " array([27, 29]),\n",
       " array([309, 311]),\n",
       " array([51, 53]),\n",
       " array([428, 430]),\n",
       " array([456, 458]),\n",
       " array([479, 481]),\n",
       " array([513, 515]),\n",
       " array([554, 556]),\n",
       " array([552, 554]),\n",
       " array([586, 588]),\n",
       " array([626, 628]),\n",
       " array([723, 725]),\n",
       " array([456, 458]),\n",
       " array([479, 481]),\n",
       " array([486, 488]),\n",
       " array([586, 588]),\n",
       " array([623, 625]),\n",
       " array([723, 725]),\n",
       " array([104, 108]),\n",
       " array([104, 108]),\n",
       " array([160, 162]),\n",
       " array([186, 188]),\n",
       " array([312, 314]),\n",
       " array([317, 319]),\n",
       " array([393, 396]),\n",
       " array([459, 465]),\n",
       " array([41, 43]),\n",
       " array([89, 91]),\n",
       " array([264, 266]),\n",
       " array([332, 334]),\n",
       " array([371, 373]),\n",
       " array([377, 379]),\n",
       " array([387, 389]),\n",
       " array([405, 407]),\n",
       " array([408, 410]),\n",
       " array([431, 433]),\n",
       " array([484, 486]),\n",
       " array([492, 494]),\n",
       " array([511, 513]),\n",
       " array([513, 515]),\n",
       " array([541, 543]),\n",
       " array([547, 549]),\n",
       " array([803, 805]),\n",
       " array([804, 806]),\n",
       " array([1029, 1031]),\n",
       " array([1203, 1205]),\n",
       " array([1287, 1289]),\n",
       " array([1348, 1351]),\n",
       " array([1373, 1375]),\n",
       " array([1393, 1395]),\n",
       " array([1429, 1431]),\n",
       " array([1445, 1447]),\n",
       " array([1641, 1643]),\n",
       " array([1792, 1795]),\n",
       " array([1830, 1832]),\n",
       " array([1687, 1689]),\n",
       " array([117, 119]),\n",
       " array([823, 825]),\n",
       " array([898, 900]),\n",
       " array([920, 922]),\n",
       " array([976, 978]),\n",
       " array([1300, 1302]),\n",
       " array([1332, 1334]),\n",
       " array([1382, 1384]),\n",
       " array([1442, 1444]),\n",
       " array([1497, 1501]),\n",
       " array([1650, 1652]),\n",
       " array([1692, 1694]),\n",
       " array([1695, 1697]),\n",
       " array([1695, 1697]),\n",
       " array([1733, 1735]),\n",
       " array([1733, 1735]),\n",
       " array([444, 446]),\n",
       " array([1097, 1099]),\n",
       " array([389, 391]),\n",
       " array([389, 391]),\n",
       " array([375, 379]),\n",
       " array([375, 379]),\n",
       " array([318, 320]),\n",
       " array([319, 321]),\n",
       " array([341, 343]),\n",
       " array([341, 343]),\n",
       " array([348, 350]),\n",
       " array([341, 343]),\n",
       " array([341, 343]),\n",
       " array([348, 350]),\n",
       " array([475, 477]),\n",
       " array([419, 421]),\n",
       " array([434, 437]),\n",
       " array([194, 196]),\n",
       " array([194, 196]),\n",
       " array([65, 67]),\n",
       " array([65, 67]),\n",
       " array([123, 125]),\n",
       " array([123, 125]),\n",
       " array([163, 165]),\n",
       " array([163, 165]),\n",
       " array([205, 207]),\n",
       " array([1710, 1712]),\n",
       " array([1710, 1712]),\n",
       " array([10, 12]),\n",
       " array([10, 12]),\n",
       " array([163, 165]),\n",
       " array([219, 221]),\n",
       " array([219, 221]),\n",
       " array([235, 237]),\n",
       " array([352, 354]),\n",
       " array([10, 12]),\n",
       " array([10, 12]),\n",
       " array([203, 205]),\n",
       " array([97, 99]),\n",
       " array([103, 106]),\n",
       " array([189, 195]),\n",
       " array([325, 329]),\n",
       " array([325, 329]),\n",
       " array([325, 329]),\n",
       " array([325, 329]),\n",
       " array([325, 329]),\n",
       " array([351, 353]),\n",
       " array([221, 224]),\n",
       " array([223, 225]),\n",
       " array([244, 246]),\n",
       " array([244, 246]),\n",
       " array([529, 531]),\n",
       " array([579, 581]),\n",
       " array([678, 680]),\n",
       " array([945, 947]),\n",
       " array([959, 961]),\n",
       " array([959, 961]),\n",
       " array([959, 961]),\n",
       " array([959, 961]),\n",
       " array([219, 221]),\n",
       " array([277, 279]),\n",
       " array([184, 186]),\n",
       " array([840, 842]),\n",
       " array([1424, 1427]),\n",
       " array([501, 504]),\n",
       " array([1256, 1258]),\n",
       " array([88, 90]),\n",
       " array([494, 496]),\n",
       " array([1171, 1174]),\n",
       " array([473, 476]),\n",
       " array([362, 364]),\n",
       " array([763, 765]),\n",
       " array([861, 863]),\n",
       " array([913, 915]),\n",
       " array([295, 298]),\n",
       " array([185, 187]),\n",
       " array([185, 187]),\n",
       " array([338, 340]),\n",
       " array([338, 340]),\n",
       " array([338, 340]),\n",
       " array([338, 340]),\n",
       " array([338, 340]),\n",
       " array([418, 420]),\n",
       " array([418, 420]),\n",
       " array([732, 734]),\n",
       " array([646, 648]),\n",
       " array([87, 89]),\n",
       " array([146, 148]),\n",
       " array([146, 148]),\n",
       " array([255, 258]),\n",
       " array([255, 258]),\n",
       " array([380, 382]),\n",
       " array([528, 530]),\n",
       " array([543, 546]),\n",
       " array([570, 572]),\n",
       " array([643, 645]),\n",
       " array([662, 664]),\n",
       " array([960, 963]),\n",
       " array([2092, 2094]),\n",
       " array([2178, 2180]),\n",
       " array([820, 822]),\n",
       " array([49, 52]),\n",
       " array([463, 466]),\n",
       " array([507, 509]),\n",
       " array([624, 627]),\n",
       " array([630, 632]),\n",
       " array([164, 168]),\n",
       " array([927, 929]),\n",
       " array([125, 128]),\n",
       " array([743, 746]),\n",
       " array([749, 751]),\n",
       " array([1, 3]),\n",
       " array([227, 229]),\n",
       " array([259, 261]),\n",
       " array([378, 380]),\n",
       " array([243, 245]),\n",
       " array([435, 437]),\n",
       " array([530, 532]),\n",
       " array([883, 885]),\n",
       " array([1100, 1102]),\n",
       " array([64, 66]),\n",
       " array([53, 55]),\n",
       " array([147, 149]),\n",
       " array([153, 155]),\n",
       " array([153, 155]),\n",
       " array([21, 23]),\n",
       " array([399, 401]),\n",
       " array([494, 498]),\n",
       " array([1032, 1034]),\n",
       " array([494, 498]),\n",
       " array([153, 155]),\n",
       " array([153, 155]),\n",
       " array([639, 642]),\n",
       " array([1183, 1185]),\n",
       " array([28, 30]),\n",
       " array([509, 511]),\n",
       " array([525, 527]),\n",
       " array([550, 553]),\n",
       " array([660, 664]),\n",
       " array([660, 664]),\n",
       " array([949, 951]),\n",
       " array([50, 54]),\n",
       " array([741, 744]),\n",
       " array([362, 364]),\n",
       " array([950, 952]),\n",
       " array([923, 925]),\n",
       " array([118, 121]),\n",
       " array([771, 773]),\n",
       " array([885, 888]),\n",
       " array([741, 744]),\n",
       " array([513, 516]),\n",
       " array([1099, 1101]),\n",
       " array([1122, 1124]),\n",
       " array([242, 244]),\n",
       " array([288, 290]),\n",
       " array([544, 546]),\n",
       " array([109, 111]),\n",
       " array([668, 670]),\n",
       " array([84, 87]),\n",
       " array([1084, 1086]),\n",
       " array([1360, 1362]),\n",
       " array([1552, 1555]),\n",
       " array([2317, 2319]),\n",
       " array([2313, 2315]),\n",
       " array([2412, 2414]),\n",
       " array([2440, 2442]),\n",
       " array([92, 94]),\n",
       " array([1823, 1826]),\n",
       " array([1823, 1826]),\n",
       " array([152, 155]),\n",
       " array([118, 120]),\n",
       " array([468, 471]),\n",
       " array([42, 44]),\n",
       " array([65, 67]),\n",
       " array([164, 166]),\n",
       " array([164, 166]),\n",
       " array([237, 239]),\n",
       " array([248, 250]),\n",
       " array([939, 942]),\n",
       " array([759, 761]),\n",
       " array([11, 13]),\n",
       " array([100, 102]),\n",
       " array([11, 13]),\n",
       " array([66, 68]),\n",
       " array([66, 68]),\n",
       " array([471, 473]),\n",
       " array([66, 68]),\n",
       " array([66, 68]),\n",
       " array([1093, 1098]),\n",
       " array([0, 3]),\n",
       " array([4, 6]),\n",
       " array([12, 15]),\n",
       " array([16, 18]),\n",
       " array([49, 51]),\n",
       " array([46, 49]),\n",
       " array([117, 119]),\n",
       " array([201, 203]),\n",
       " array([437, 439]),\n",
       " array([610, 612]),\n",
       " array([714, 716]),\n",
       " array([737, 739]),\n",
       " array([752, 754]),\n",
       " array([770, 772]),\n",
       " array([820, 822]),\n",
       " array([1283, 1285]),\n",
       " array([1485, 1487]),\n",
       " array([1554, 1556]),\n",
       " array([300, 302]),\n",
       " array([11907, 11912]),\n",
       " array([11891, 11894]),\n",
       " array([11822, 11825]),\n",
       " array([11246, 11248]),\n",
       " array([9804, 9806]),\n",
       " array([11519, 11522]),\n",
       " array([11464, 11466]),\n",
       " array([10655, 10658]),\n",
       " array([9725, 9727]),\n",
       " array([8208, 8210]),\n",
       " array([7289, 7293]),\n",
       " array([7327, 7330]),\n",
       " array([7335, 7337]),\n",
       " array([3578, 3581]),\n",
       " array([2637, 2639]),\n",
       " array([2475, 2477]),\n",
       " array([1810, 1813]),\n",
       " array([1234, 1236]),\n",
       " array([6126, 6130]),\n",
       " array([1810, 1813]),\n",
       " array([1234, 1236]),\n",
       " array([1234, 1236]),\n",
       " array([1234, 1236]),\n",
       " array([10784, 10786]),\n",
       " array([10244, 10246]),\n",
       " array([10092, 10094]),\n",
       " array([10186, 10188]),\n",
       " array([9398, 9400]),\n",
       " array([9266, 9268]),\n",
       " array([9425, 9427]),\n",
       " array([8723, 8725]),\n",
       " array([7474, 7476]),\n",
       " array([5769, 5771]),\n",
       " array([4483, 4485]),\n",
       " array([206, 208]),\n",
       " array([490, 492]),\n",
       " array([215, 217]),\n",
       " array([445, 448]),\n",
       " array([266, 268]),\n",
       " array([304, 310]),\n",
       " array([923, 926]),\n",
       " array([473, 475]),\n",
       " array([587, 589]),\n",
       " array([280, 282]),\n",
       " array([676, 679]),\n",
       " array([749, 754]),\n",
       " array([809, 812]),\n",
       " array([80, 82]),\n",
       " array([108, 110]),\n",
       " array([544, 546]),\n",
       " array([362, 366]),\n",
       " array([689, 691]),\n",
       " array([638, 640]),\n",
       " array([638, 640]),\n",
       " array([763, 765]),\n",
       " array([815, 817]),\n",
       " array([66, 68]),\n",
       " array([139, 142]),\n",
       " array([203, 205]),\n",
       " array([423, 425]),\n",
       " array([362, 364]),\n",
       " array([706, 708]),\n",
       " array([43, 45]),\n",
       " array([895, 897]),\n",
       " array([993, 995]),\n",
       " array([81, 84]),\n",
       " array([117, 119]),\n",
       " array([450, 452]),\n",
       " array([810, 814]),\n",
       " array([493, 496]),\n",
       " array([359, 361]),\n",
       " array([289, 292]),\n",
       " array([93, 95]),\n",
       " array([196, 198]),\n",
       " array([834, 836]),\n",
       " array([ 9, 11]),\n",
       " array([1174, 1176]),\n",
       " array([1347, 1349]),\n",
       " array([216, 219]),\n",
       " array([74, 76]),\n",
       " array([124, 126]),\n",
       " array([567, 570]),\n",
       " array([573, 575]),\n",
       " array([656, 658]),\n",
       " array([719, 721]),\n",
       " array([637, 639]),\n",
       " array([772, 774]),\n",
       " array([782, 785]),\n",
       " array([812, 814]),\n",
       " array([815, 817]),\n",
       " array([875, 877]),\n",
       " array([884, 886]),\n",
       " array([ 999, 1001]),\n",
       " array([1040, 1045]),\n",
       " array([1094, 1096]),\n",
       " array([1127, 1129]),\n",
       " array([17, 19]),\n",
       " array([1258, 1263]),\n",
       " array([1154, 1156]),\n",
       " array([1357, 1359]),\n",
       " array([37, 42]),\n",
       " array([1569, 1571]),\n",
       " array([624, 626]),\n",
       " array([885, 887]),\n",
       " array([1007, 1009]),\n",
       " array([1040, 1042]),\n",
       " array([1325, 1327]),\n",
       " array([1233, 1235]),\n",
       " array([201, 203]),\n",
       " array([234, 236]),\n",
       " array([268, 270]),\n",
       " array([285, 287]),\n",
       " array([307, 309]),\n",
       " array([309, 311]),\n",
       " array([359, 361]),\n",
       " array([ 997, 1001]),\n",
       " array([1086, 1089]),\n",
       " array([1120, 1122]),\n",
       " array([1133, 1137]),\n",
       " array([1530, 1532]),\n",
       " array([1542, 1544]),\n",
       " array([203, 205]),\n",
       " array([214, 216]),\n",
       " array([268, 270]),\n",
       " array([385, 387]),\n",
       " array([413, 419]),\n",
       " array([590, 593]),\n",
       " array([878, 880]),\n",
       " array([960, 962]),\n",
       " array([1052, 1054]),\n",
       " array([1120, 1123]),\n",
       " array([1126, 1128]),\n",
       " array([1325, 1327]),\n",
       " array([1399, 1401]),\n",
       " array([2016, 2018]),\n",
       " array([2079, 2081]),\n",
       " array([2135, 2137]),\n",
       " array([474, 476]),\n",
       " array([2098, 2104]),\n",
       " array([426, 448]),\n",
       " array([584, 597]),\n",
       " array([821, 833]),\n",
       " array([1037, 1045]),\n",
       " array([1097, 1101]),\n",
       " array([1112, 1120]),\n",
       " array([1207, 1214]),\n",
       " array([1215, 1227]),\n",
       " array([1373, 1376]),\n",
       " array([1705, 1711]),\n",
       " array([1948, 1965]),\n",
       " array([2015, 2023]),\n",
       " array([2033, 2047]),\n",
       " array([2064, 2085]),\n",
       " array([2115, 2129]),\n",
       " array([2133, 2146]),\n",
       " array([2163, 2178]),\n",
       " array([2269, 2277]),\n",
       " array([2386, 2402]),\n",
       " array([2441, 2457]),\n",
       " array([2552, 2578]),\n",
       " array([2613, 2634]),\n",
       " array([2644, 2678]),\n",
       " array([2644, 2678]),\n",
       " array([2678, 2688]),\n",
       " array([2688, 2714]),\n",
       " array([2715, 2734]),\n",
       " array([2781, 2796]),\n",
       " array([2890, 2911]),\n",
       " array([2927, 2975]),\n",
       " array([3055, 3065]),\n",
       " array([3069, 3082]),\n",
       " array([3124, 3142]),\n",
       " array([3190, 3207]),\n",
       " array([3305, 3314]),\n",
       " array([3586, 3604]),\n",
       " array([3619, 3671]),\n",
       " array([3714, 3745]),\n",
       " array([3876, 3886]),\n",
       " array([3976, 3994]),\n",
       " array([4017, 4072]),\n",
       " array([4556, 4575]),\n",
       " array([4667, 4690]),\n",
       " array([4713, 4737]),\n",
       " array([4748, 4767]),\n",
       " array([5379, 5388]),\n",
       " array([5437, 5455]),\n",
       " array([5543, 5569]),\n",
       " array([5649, 5651]),\n",
       " array([5658, 5681]),\n",
       " array([6268, 6282]),\n",
       " array([6303, 6305]),\n",
       " array([6324, 6326]),\n",
       " array([528, 530]),\n",
       " array([1204, 1206]),\n",
       " array([1228, 1230]),\n",
       " array([1215, 1217]),\n",
       " array([66, 70]),\n",
       " array([535, 537]),\n",
       " array([1030, 1032]),\n",
       " array([1030, 1032]),\n",
       " array([528, 530]),\n",
       " array([1040, 1042]),\n",
       " array([1175, 1177]),\n",
       " array([1196, 1198]),\n",
       " array([1387, 1389]),\n",
       " array([392, 394]),\n",
       " array([1228, 1230]),\n",
       " array([51, 57]),\n",
       " array([32, 34]),\n",
       " array([89, 91]),\n",
       " array([186, 188]),\n",
       " array([379, 386]),\n",
       " array([402, 404]),\n",
       " array([420, 422]),\n",
       " array([767, 769]),\n",
       " array([774, 776]),\n",
       " array([792, 794]),\n",
       " array([79, 81]),\n",
       " array([79, 81]),\n",
       " array([104, 106]),\n",
       " array([181, 183]),\n",
       " array([183, 185]),\n",
       " array([220, 222]),\n",
       " array([104, 106]),\n",
       " array([132, 134]),\n",
       " array([24, 26]),\n",
       " array([24, 26]),\n",
       " array([18, 20]),\n",
       " array([181, 183]),\n",
       " array([183, 185]),\n",
       " array([6, 8]),\n",
       " array([117, 119]),\n",
       " array([120, 122]),\n",
       " array([141, 144]),\n",
       " array([206, 210]),\n",
       " array([238, 243]),\n",
       " array([245, 248]),\n",
       " array([430, 434]),\n",
       " array([596, 598]),\n",
       " array([658, 660]),\n",
       " array([697, 699]),\n",
       " array([813, 815]),\n",
       " array([933, 936]),\n",
       " array([982, 984]),\n",
       " array([1131, 1133]),\n",
       " array([1313, 1316]),\n",
       " array([1641, 1646]),\n",
       " array([1674, 1677]),\n",
       " array([1777, 1780]),\n",
       " array([1800, 1803]),\n",
       " array([1806, 1808]),\n",
       " array([1840, 1846]),\n",
       " array([2128, 2132]),\n",
       " array([228, 230]),\n",
       " array([243, 245]),\n",
       " array([248, 250]),\n",
       " array([245, 247]),\n",
       " array([6, 8]),\n",
       " array([356, 358]),\n",
       " array([356, 358]),\n",
       " array([48, 50]),\n",
       " array([198, 200]),\n",
       " array([198, 200]),\n",
       " array([127, 129]),\n",
       " array([143, 146]),\n",
       " array([143, 146]),\n",
       " array([210, 212]),\n",
       " array([187, 189]),\n",
       " array([217, 221]),\n",
       " array([239, 241]),\n",
       " array([244, 246]),\n",
       " array([433, 435]),\n",
       " array([433, 435]),\n",
       " array([228, 230]),\n",
       " array([226, 228]),\n",
       " array([231, 236]),\n",
       " array([226, 228]),\n",
       " array([774, 776]),\n",
       " array([784, 786]),\n",
       " array([199, 202]),\n",
       " array([49, 51]),\n",
       " array([18, 20]),\n",
       " array([260, 262]),\n",
       " array([313, 315]),\n",
       " array([517, 519]),\n",
       " array([520, 522]),\n",
       " array([526, 528]),\n",
       " array([418, 420]),\n",
       " array([421, 423]),\n",
       " array([660, 662]),\n",
       " array([722, 724]),\n",
       " array([1190, 1192]),\n",
       " array([1216, 1218]),\n",
       " array([1130, 1133]),\n",
       " array([197, 199]),\n",
       " array([205, 207]),\n",
       " array([329, 331]),\n",
       " array([46, 48]),\n",
       " array([38, 40]),\n",
       " array([274, 276]),\n",
       " array([324, 326]),\n",
       " array([449, 451]),\n",
       " array([497, 500]),\n",
       " array([518, 520]),\n",
       " array([522, 527]),\n",
       " array([731, 733]),\n",
       " array([546, 548]),\n",
       " array([546, 548]),\n",
       " array([566, 568]),\n",
       " array([581, 583]),\n",
       " array([585, 587]),\n",
       " array([608, 611]),\n",
       " array([634, 636]),\n",
       " array([681, 683]),\n",
       " array([683, 685]),\n",
       " array([13, 15]),\n",
       " array([13, 15]),\n",
       " array([56, 58]),\n",
       " array([1550, 1553]),\n",
       " array([1572, 1574]),\n",
       " array([1578, 1580]),\n",
       " array([1590, 1592]),\n",
       " array([1605, 1607]),\n",
       " array([1691, 1693]),\n",
       " array([1693, 1696]),\n",
       " array([1755, 1758]),\n",
       " array([1761, 1765]),\n",
       " array([1789, 1792]),\n",
       " array([1811, 1816]),\n",
       " array([1816, 1818]),\n",
       " array([1835, 1837]),\n",
       " array([1912, 1919]),\n",
       " array([1962, 1964]),\n",
       " array([2545, 2547]),\n",
       " array([2939, 2941]),\n",
       " array([3002, 3004]),\n",
       " array([3107, 3109]),\n",
       " array([3176, 3178]),\n",
       " array([3178, 3180]),\n",
       " array([3185, 3187]),\n",
       " array([3382, 3387]),\n",
       " array([4297, 4299]),\n",
       " array([4537, 4541]),\n",
       " array([6077, 6079]),\n",
       " array([6442, 6444]),\n",
       " array([6465, 6467]),\n",
       " array([6499, 6501]),\n",
       " array([6969, 6972]),\n",
       " array([7419, 7424]),\n",
       " array([7423, 7425]),\n",
       " array([7547, 7549]),\n",
       " array([7547, 7549]),\n",
       " array([7597, 7599]),\n",
       " array([7603, 7605]),\n",
       " array([7603, 7605]),\n",
       " array([46, 48]),\n",
       " array([178, 180]),\n",
       " array([169, 171]),\n",
       " array([219, 222]),\n",
       " array([219, 222]),\n",
       " array([243, 245]),\n",
       " array([251, 253]),\n",
       " array([283, 285]),\n",
       " array([283, 285]),\n",
       " array([424, 426]),\n",
       " array([435, 437]),\n",
       " array([441, 443]),\n",
       " array([707, 709]),\n",
       " array([708, 710]),\n",
       " array([402, 404]),\n",
       " array([453, 456]),\n",
       " array([469, 471]),\n",
       " array([484, 486]),\n",
       " array([493, 495]),\n",
       " array([751, 753]),\n",
       " array([799, 801]),\n",
       " array([837, 839]),\n",
       " array([132, 135]),\n",
       " array([148, 150]),\n",
       " array([101, 104]),\n",
       " array([169, 171]),\n",
       " array([218, 221]),\n",
       " array([293, 295]),\n",
       " array([272, 274]),\n",
       " array([356, 358]),\n",
       " array([342, 344]),\n",
       " array([421, 423]),\n",
       " array([445, 447]),\n",
       " array([478, 480]),\n",
       " array([494, 496]),\n",
       " array([514, 516]),\n",
       " array([531, 533]),\n",
       " array([564, 566]),\n",
       " array([564, 566]),\n",
       " array([614, 616]),\n",
       " array([614, 616]),\n",
       " array([712, 714]),\n",
       " array([643, 645]),\n",
       " array([39, 43]),\n",
       " array([46, 48]),\n",
       " array([79, 81]),\n",
       " array([356, 358]),\n",
       " array([456, 458]),\n",
       " array([110, 112]),\n",
       " array([158, 160]),\n",
       " array([339, 341]),\n",
       " array([511, 513]),\n",
       " array([100, 102]),\n",
       " array([60, 62]),\n",
       " array([ 9, 11]),\n",
       " array([304, 306]),\n",
       " array([605, 607]),\n",
       " array([79, 81]),\n",
       " array([93, 95]),\n",
       " array([252, 254]),\n",
       " array([252, 254]),\n",
       " array([127, 129]),\n",
       " array([179, 181]),\n",
       " array([215, 217]),\n",
       " array([13, 15]),\n",
       " array([259, 261]),\n",
       " array([427, 429]),\n",
       " array([500, 502]),\n",
       " array([479, 481]),\n",
       " array([244, 246]),\n",
       " array([244, 246]),\n",
       " array([247, 249]),\n",
       " array([247, 249]),\n",
       " array([289, 291]),\n",
       " array([289, 291]),\n",
       " array([584, 586]),\n",
       " array([549, 551]),\n",
       " array([549, 551]),\n",
       " array([584, 586]),\n",
       " array([456, 458]),\n",
       " array([454, 456]),\n",
       " array([366, 368]),\n",
       " array([391, 396]),\n",
       " array([485, 487]),\n",
       " array([138, 140]),\n",
       " array([138, 140]),\n",
       " array([59, 61]),\n",
       " array([71, 73]),\n",
       " array([444, 447]),\n",
       " array([457, 459]),\n",
       " array([463, 465]),\n",
       " array([467, 469]),\n",
       " array([403, 405]),\n",
       " array([413, 417]),\n",
       " array([55, 58]),\n",
       " array([93, 96]),\n",
       " array([123, 125]),\n",
       " array([150, 154]),\n",
       " array([150, 154]),\n",
       " array([171, 174]),\n",
       " array([182, 190]),\n",
       " array([207, 210]),\n",
       " array([210, 214]),\n",
       " array([791, 795]),\n",
       " array([18, 23]),\n",
       " array([223, 225]),\n",
       " array([208, 211]),\n",
       " array([174, 176]),\n",
       " array([196, 198]),\n",
       " array([240, 242]),\n",
       " array([257, 259]),\n",
       " array([271, 273]),\n",
       " array([286, 288]),\n",
       " array([278, 280]),\n",
       " array([1, 3]),\n",
       " array([178, 180]),\n",
       " array([1, 3]),\n",
       " array([178, 180]),\n",
       " array([521, 523]),\n",
       " array([527, 529]),\n",
       " array([515, 517])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_mentions = []\n",
    "bert_sentences\n",
    "batch = [bert_sentences[x] for x in range( len(bert_sentences))]\n",
    " \n",
    "bert_tokens = parallel_coref_model.module.tokenizer(batch, pad_to_max_length=True, add_special_tokens=False, truncation =False )\n",
    "#bert_tokens = parallel_model.module.tokenizer(batch, padding = 'max_length', add_special_tokens=False, truncation =True )\n",
    "input_ids = torch.tensor(bert_tokens['input_ids'], device=device)\n",
    "attention_mask = torch.tensor(bert_tokens['attention_mask'], device=device)\n",
    "m = input_ids.cpu()\n",
    "k = m == parallel_coref_model.module.vals[0]\n",
    "p = m == parallel_coref_model.module.vals[1]\n",
    "v = (k.int() + p.int()).bool()\n",
    "nz_indexes = v.nonzero()[:, 1].reshape(m.shape[0], 2)\n",
    "print(nz_indexes.shape)\n",
    "#print(nz_indexes[0][1])\n",
    "loc_mentions.extend(nz_indexes.cpu().numpy())\n",
    "print(input_ids.shape)\n",
    "loc_mentions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ba21077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 666]\n",
      "m shape torch.Size([512]) 4\n",
      "740\n",
      "[6, 750]\n",
      "m shape torch.Size([512]) 6\n",
      "740\n",
      "[7, 750]\n",
      "m shape torch.Size([512]) 7\n",
      "740\n",
      "[8, 924]\n",
      "m shape torch.Size([512]) 8\n",
      "740\n",
      "[9, 965]\n",
      "m shape torch.Size([512]) 9\n",
      "740\n",
      "[10, 1001]\n",
      "m shape torch.Size([512]) 10\n",
      "741\n",
      "[11, 1001]\n",
      "m shape torch.Size([512]) 11\n",
      "741\n",
      "[12, 1030]\n",
      "m shape torch.Size([512]) 12\n",
      "741\n",
      "[13, 1027]\n",
      "m shape torch.Size([512]) 13\n",
      "740\n",
      "[14, 1060]\n",
      "m shape torch.Size([512]) 14\n",
      "740\n",
      "[15, 1060]\n",
      "m shape torch.Size([512]) 15\n",
      "740\n",
      "[16, 1105]\n",
      "m shape torch.Size([512]) 16\n",
      "740\n",
      "[17, 1137]\n",
      "m shape torch.Size([512]) 17\n",
      "740\n",
      "[18, 1137]\n",
      "m shape torch.Size([512]) 18\n",
      "740\n",
      "[19, 1150]\n",
      "m shape torch.Size([512]) 19\n",
      "740\n",
      "[20, 1178]\n",
      "m shape torch.Size([512]) 20\n",
      "740\n",
      "[21, 1194]\n",
      "m shape torch.Size([512]) 21\n",
      "740\n",
      "[22, 1215]\n",
      "m shape torch.Size([512]) 22\n",
      "740\n",
      "[23, 1218]\n",
      "m shape torch.Size([512]) 23\n",
      "740\n",
      "[25, 666]\n",
      "m shape torch.Size([512]) 25\n",
      "740\n",
      "[26, 563]\n",
      "m shape torch.Size([512]) 26\n",
      "740\n",
      "[27, 1043]\n",
      "m shape torch.Size([512]) 27\n",
      "741\n",
      "[28, 1064]\n",
      "m shape torch.Size([512]) 28\n",
      "741\n",
      "[29, 1195]\n",
      "m shape torch.Size([512]) 29\n",
      "741\n",
      "[34, 610]\n",
      "m shape torch.Size([512]) 34\n",
      "721\n",
      "[35, 626]\n",
      "m shape torch.Size([512]) 35\n",
      "721\n",
      "[36, 646]\n",
      "m shape torch.Size([512]) 36\n",
      "722\n",
      "[38, 617]\n",
      "m shape torch.Size([512]) 38\n",
      "721\n",
      "[39, 633]\n",
      "m shape torch.Size([512]) 39\n",
      "721\n",
      "[40, 646]\n",
      "m shape torch.Size([512]) 40\n",
      "722\n",
      "[66, 512]\n",
      "m shape torch.Size([512]) 66\n",
      "419\n",
      "[67, 518]\n",
      "m shape torch.Size([512]) 67\n",
      "419\n",
      "[68, 522]\n",
      "m shape torch.Size([512]) 68\n",
      "418\n",
      "[69, 528]\n",
      "m shape torch.Size([512]) 69\n",
      "418\n",
      "[70, 544]\n",
      "m shape torch.Size([512]) 70\n",
      "418\n",
      "[71, 546]\n",
      "m shape torch.Size([512]) 71\n",
      "418\n",
      "[72, 649]\n",
      "m shape torch.Size([512]) 72\n",
      "418\n",
      "[73, 660]\n",
      "m shape torch.Size([512]) 73\n",
      "418\n",
      "[74, 665]\n",
      "m shape torch.Size([512]) 74\n",
      "418\n",
      "[97, 515]\n",
      "m shape torch.Size([512]) 97\n",
      "767\n",
      "[98, 556]\n",
      "m shape torch.Size([512]) 98\n",
      "768\n",
      "[99, 554]\n",
      "m shape torch.Size([512]) 99\n",
      "767\n",
      "[100, 588]\n",
      "m shape torch.Size([512]) 100\n",
      "767\n",
      "[101, 628]\n",
      "m shape torch.Size([512]) 101\n",
      "767\n",
      "[102, 725]\n",
      "m shape torch.Size([512]) 102\n",
      "767\n",
      "[106, 588]\n",
      "m shape torch.Size([512]) 106\n",
      "767\n",
      "[107, 625]\n",
      "m shape torch.Size([512]) 107\n",
      "767\n",
      "[108, 725]\n",
      "m shape torch.Size([512]) 108\n",
      "767\n",
      "[129, 513]\n",
      "m shape torch.Size([512]) 129\n",
      "1353\n",
      "[130, 515]\n",
      "m shape torch.Size([512]) 130\n",
      "1353\n",
      "[131, 543]\n",
      "m shape torch.Size([512]) 131\n",
      "1353\n",
      "[132, 549]\n",
      "m shape torch.Size([512]) 132\n",
      "1353\n",
      "[133, 805]\n",
      "m shape torch.Size([512]) 133\n",
      "1353\n",
      "[134, 806]\n",
      "m shape torch.Size([512]) 134\n",
      "1353\n",
      "[135, 1031]\n",
      "m shape torch.Size([512]) 135\n",
      "1353\n",
      "[136, 1205]\n",
      "m shape torch.Size([512]) 136\n",
      "1353\n",
      "[137, 1289]\n",
      "m shape torch.Size([512]) 137\n",
      "1353\n",
      "[138, 1351]\n",
      "m shape torch.Size([512]) 138\n",
      "1353\n",
      "[139, 1375]\n",
      "m shape torch.Size([512]) 139\n",
      "1353\n",
      "[140, 1395]\n",
      "m shape torch.Size([512]) 140\n",
      "1353\n",
      "[141, 1431]\n",
      "m shape torch.Size([512]) 141\n",
      "1353\n",
      "[142, 1447]\n",
      "m shape torch.Size([512]) 142\n",
      "1353\n",
      "[143, 1643]\n",
      "m shape torch.Size([512]) 143\n",
      "1354\n",
      "[144, 1795]\n",
      "m shape torch.Size([512]) 144\n",
      "1353\n",
      "[145, 1832]\n",
      "m shape torch.Size([512]) 145\n",
      "1354\n",
      "[146, 1689]\n",
      "m shape torch.Size([512]) 146\n",
      "1353\n",
      "[148, 825]\n",
      "m shape torch.Size([512]) 148\n",
      "1264\n",
      "[149, 900]\n",
      "m shape torch.Size([512]) 149\n",
      "1264\n",
      "[150, 922]\n",
      "m shape torch.Size([512]) 150\n",
      "1264\n",
      "[151, 978]\n",
      "m shape torch.Size([512]) 151\n",
      "1264\n",
      "[152, 1302]\n",
      "m shape torch.Size([512]) 152\n",
      "1265\n",
      "[153, 1334]\n",
      "m shape torch.Size([512]) 153\n",
      "1264\n",
      "[154, 1384]\n",
      "m shape torch.Size([512]) 154\n",
      "1264\n",
      "[155, 1444]\n",
      "m shape torch.Size([512]) 155\n",
      "1264\n",
      "[156, 1501]\n",
      "m shape torch.Size([512]) 156\n",
      "1264\n",
      "[157, 1652]\n",
      "m shape torch.Size([512]) 157\n",
      "1265\n",
      "[158, 1694]\n",
      "m shape torch.Size([512]) 158\n",
      "1264\n",
      "[159, 1697]\n",
      "m shape torch.Size([512]) 159\n",
      "1264\n",
      "[160, 1697]\n",
      "m shape torch.Size([512]) 160\n",
      "1264\n",
      "[161, 1735]\n",
      "m shape torch.Size([512]) 161\n",
      "1264\n",
      "[162, 1735]\n",
      "m shape torch.Size([512]) 162\n",
      "1264\n",
      "[164, 1099]\n",
      "m shape torch.Size([512]) 164\n",
      "1264\n",
      "[189, 1712]\n",
      "m shape torch.Size([512]) 189\n",
      "1213\n",
      "[190, 1712]\n",
      "m shape torch.Size([512]) 190\n",
      "1213\n",
      "[214, 531]\n",
      "m shape torch.Size([512]) 214\n",
      "743\n",
      "[215, 581]\n",
      "m shape torch.Size([512]) 215\n",
      "743\n",
      "[216, 680]\n",
      "m shape torch.Size([512]) 216\n",
      "743\n",
      "[217, 947]\n",
      "m shape torch.Size([512]) 217\n",
      "743\n",
      "[218, 961]\n",
      "m shape torch.Size([512]) 218\n",
      "743\n",
      "[219, 961]\n",
      "m shape torch.Size([512]) 219\n",
      "743\n",
      "[220, 961]\n",
      "m shape torch.Size([512]) 220\n",
      "743\n",
      "[221, 961]\n",
      "m shape torch.Size([512]) 221\n",
      "743\n",
      "[225, 842]\n",
      "m shape torch.Size([512]) 225\n",
      "743\n",
      "[226, 1427]\n",
      "m shape torch.Size([512]) 226\n",
      "2180\n",
      "[228, 1258]\n",
      "m shape torch.Size([512]) 228\n",
      "2180\n",
      "[231, 1174]\n",
      "m shape torch.Size([512]) 231\n",
      "1011\n",
      "[234, 765]\n",
      "m shape torch.Size([512]) 234\n",
      "759\n",
      "[235, 863]\n",
      "m shape torch.Size([512]) 235\n",
      "759\n",
      "[236, 915]\n",
      "m shape torch.Size([512]) 236\n",
      "759\n",
      "[247, 734]\n",
      "m shape torch.Size([512]) 247\n",
      "759\n",
      "[248, 648]\n",
      "m shape torch.Size([512]) 248\n",
      "759\n",
      "[255, 530]\n",
      "m shape torch.Size([512]) 255\n",
      "1775\n",
      "[256, 546]\n",
      "m shape torch.Size([512]) 256\n",
      "1775\n",
      "[257, 572]\n",
      "m shape torch.Size([512]) 257\n",
      "1775\n",
      "[258, 645]\n",
      "m shape torch.Size([512]) 258\n",
      "1775\n",
      "[259, 664]\n",
      "m shape torch.Size([512]) 259\n",
      "1776\n",
      "[260, 963]\n",
      "m shape torch.Size([512]) 260\n",
      "1775\n",
      "[261, 2094]\n",
      "m shape torch.Size([512]) 261\n",
      "1775\n",
      "[262, 2180]\n",
      "m shape torch.Size([512]) 262\n",
      "1776\n",
      "[263, 822]\n",
      "m shape torch.Size([512]) 263\n",
      "1775\n",
      "[267, 627]\n",
      "m shape torch.Size([512]) 267\n",
      "1043\n",
      "[268, 632]\n",
      "m shape torch.Size([512]) 268\n",
      "1043\n",
      "[270, 929]\n",
      "m shape torch.Size([512]) 270\n",
      "1043\n",
      "[272, 746]\n",
      "m shape torch.Size([512]) 272\n",
      "1043\n",
      "[273, 751]\n",
      "m shape torch.Size([512]) 273\n",
      "1044\n",
      "[280, 532]\n",
      "m shape torch.Size([512]) 280\n",
      "930\n",
      "[281, 885]\n",
      "m shape torch.Size([512]) 281\n",
      "930\n",
      "[282, 1102]\n",
      "m shape torch.Size([512]) 282\n",
      "930\n",
      "[291, 1034]\n",
      "m shape torch.Size([512]) 291\n",
      "1120\n",
      "[295, 642]\n",
      "m shape torch.Size([512]) 295\n",
      "1121\n",
      "[296, 1185]\n",
      "m shape torch.Size([512]) 296\n",
      "1120\n",
      "[299, 527]\n",
      "m shape torch.Size([512]) 299\n",
      "713\n",
      "[300, 553]\n",
      "m shape torch.Size([512]) 300\n",
      "713\n",
      "[301, 664]\n",
      "m shape torch.Size([512]) 301\n",
      "713\n",
      "[302, 664]\n",
      "m shape torch.Size([512]) 302\n",
      "713\n",
      "[303, 951]\n",
      "m shape torch.Size([512]) 303\n",
      "713\n",
      "[305, 744]\n",
      "m shape torch.Size([512]) 305\n",
      "713\n",
      "[307, 952]\n",
      "m shape torch.Size([512]) 307\n",
      "714\n",
      "[308, 925]\n",
      "m shape torch.Size([512]) 308\n",
      "713\n",
      "[310, 773]\n",
      "m shape torch.Size([512]) 310\n",
      "713\n",
      "[311, 888]\n",
      "m shape torch.Size([512]) 311\n",
      "713\n",
      "[312, 744]\n",
      "m shape torch.Size([512]) 312\n",
      "713\n",
      "[313, 516]\n",
      "m shape torch.Size([512]) 313\n",
      "713\n",
      "[314, 1101]\n",
      "m shape torch.Size([512]) 314\n",
      "778\n",
      "[315, 1124]\n",
      "m shape torch.Size([512]) 315\n",
      "779\n",
      "[318, 546]\n",
      "m shape torch.Size([512]) 318\n",
      "778\n",
      "[320, 670]\n",
      "m shape torch.Size([512]) 320\n",
      "778\n",
      "[322, 1086]\n",
      "m shape torch.Size([512]) 322\n",
      "1849\n",
      "[323, 1362]\n",
      "m shape torch.Size([512]) 323\n",
      "1848\n",
      "[324, 1555]\n",
      "m shape torch.Size([512]) 324\n",
      "1848\n",
      "[325, 2319]\n",
      "m shape torch.Size([512]) 325\n",
      "1848\n",
      "[326, 2315]\n",
      "m shape torch.Size([512]) 326\n",
      "1848\n",
      "[327, 2414]\n",
      "m shape torch.Size([512]) 327\n",
      "1848\n",
      "[328, 2442]\n",
      "m shape torch.Size([512]) 328\n",
      "1848\n",
      "[330, 1826]\n",
      "m shape torch.Size([512]) 330\n",
      "1848\n",
      "[331, 1826]\n",
      "m shape torch.Size([512]) 331\n",
      "1848\n",
      "[341, 942]\n",
      "m shape torch.Size([512]) 341\n",
      "690\n",
      "[342, 761]\n",
      "m shape torch.Size([512]) 342\n",
      "689\n",
      "[351, 1098]\n",
      "m shape torch.Size([512]) 351\n",
      "962\n",
      "[361, 612]\n",
      "m shape torch.Size([512]) 361\n",
      "1080\n",
      "[362, 716]\n",
      "m shape torch.Size([512]) 362\n",
      "1080\n",
      "[363, 739]\n",
      "m shape torch.Size([512]) 363\n",
      "1080\n",
      "[364, 754]\n",
      "m shape torch.Size([512]) 364\n",
      "1081\n",
      "[365, 772]\n",
      "m shape torch.Size([512]) 365\n",
      "1081\n",
      "[366, 822]\n",
      "m shape torch.Size([512]) 366\n",
      "1080\n",
      "[367, 1285]\n",
      "m shape torch.Size([512]) 367\n",
      "1080\n",
      "[368, 1487]\n",
      "m shape torch.Size([512]) 368\n",
      "1080\n",
      "[369, 1556]\n",
      "m shape torch.Size([512]) 369\n",
      "1081\n",
      "[371, 11912]\n",
      "m shape torch.Size([512]) 371\n",
      "7192\n",
      "[372, 11894]\n",
      "m shape torch.Size([512]) 372\n",
      "7192\n",
      "[373, 11825]\n",
      "m shape torch.Size([512]) 373\n",
      "7192\n",
      "[374, 11248]\n",
      "m shape torch.Size([512]) 374\n",
      "7192\n",
      "[375, 9806]\n",
      "m shape torch.Size([512]) 375\n",
      "7192\n",
      "[376, 11522]\n",
      "m shape torch.Size([512]) 376\n",
      "7192\n",
      "[377, 11466]\n",
      "m shape torch.Size([512]) 377\n",
      "7192\n",
      "[378, 10658]\n",
      "m shape torch.Size([512]) 378\n",
      "7192\n",
      "[379, 9727]\n",
      "m shape torch.Size([512]) 379\n",
      "7192\n",
      "[380, 8210]\n",
      "m shape torch.Size([512]) 380\n",
      "7192\n",
      "[381, 7293]\n",
      "m shape torch.Size([512]) 381\n",
      "7192\n",
      "[382, 7330]\n",
      "m shape torch.Size([512]) 382\n",
      "7192\n",
      "[383, 7337]\n",
      "m shape torch.Size([512]) 383\n",
      "7192\n",
      "[384, 3581]\n",
      "m shape torch.Size([512]) 384\n",
      "7192\n",
      "[385, 2639]\n",
      "m shape torch.Size([512]) 385\n",
      "7192\n",
      "[386, 2477]\n",
      "m shape torch.Size([512]) 386\n",
      "7192\n",
      "[387, 1813]\n",
      "m shape torch.Size([512]) 387\n",
      "7192\n",
      "[388, 1236]\n",
      "m shape torch.Size([512]) 388\n",
      "7192\n",
      "[389, 6130]\n",
      "m shape torch.Size([512]) 389\n",
      "7192\n",
      "[390, 1813]\n",
      "m shape torch.Size([512]) 390\n",
      "7192\n",
      "[391, 1236]\n",
      "m shape torch.Size([512]) 391\n",
      "7192\n",
      "[392, 1236]\n",
      "m shape torch.Size([512]) 392\n",
      "7192\n",
      "[393, 1236]\n",
      "m shape torch.Size([512]) 393\n",
      "7192\n",
      "[394, 10786]\n",
      "m shape torch.Size([512]) 394\n",
      "7192\n",
      "[395, 10246]\n",
      "m shape torch.Size([512]) 395\n",
      "7192\n",
      "[396, 10094]\n",
      "m shape torch.Size([512]) 396\n",
      "7192\n",
      "[397, 10188]\n",
      "m shape torch.Size([512]) 397\n",
      "7192\n",
      "[398, 9400]\n",
      "m shape torch.Size([512]) 398\n",
      "7192\n",
      "[399, 9268]\n",
      "m shape torch.Size([512]) 399\n",
      "7193\n",
      "[400, 9427]\n",
      "m shape torch.Size([512]) 400\n",
      "7193\n",
      "[401, 8725]\n",
      "m shape torch.Size([512]) 401\n",
      "7193\n",
      "[402, 7476]\n",
      "m shape torch.Size([512]) 402\n",
      "7192\n",
      "[403, 5771]\n",
      "m shape torch.Size([512]) 403\n",
      "7192\n",
      "[404, 4485]\n",
      "m shape torch.Size([512]) 404\n",
      "7192\n",
      "[411, 926]\n",
      "m shape torch.Size([512]) 411\n",
      "843\n",
      "[413, 589]\n",
      "m shape torch.Size([512]) 413\n",
      "842\n",
      "[415, 679]\n",
      "m shape torch.Size([512]) 415\n",
      "842\n",
      "[416, 754]\n",
      "m shape torch.Size([512]) 416\n",
      "842\n",
      "[417, 812]\n",
      "m shape torch.Size([512]) 417\n",
      "843\n",
      "[420, 546]\n",
      "m shape torch.Size([512]) 420\n",
      "533\n",
      "[422, 691]\n",
      "m shape torch.Size([512]) 422\n",
      "1073\n",
      "[423, 640]\n",
      "m shape torch.Size([512]) 423\n",
      "1073\n",
      "[424, 640]\n",
      "m shape torch.Size([512]) 424\n",
      "1073\n",
      "[425, 765]\n",
      "m shape torch.Size([512]) 425\n",
      "1073\n",
      "[426, 817]\n",
      "m shape torch.Size([512]) 426\n",
      "1073\n",
      "[432, 708]\n",
      "m shape torch.Size([512]) 432\n",
      "739\n",
      "[434, 897]\n",
      "m shape torch.Size([512]) 434\n",
      "961\n",
      "[435, 995]\n",
      "m shape torch.Size([512]) 435\n",
      "961\n",
      "[439, 814]\n",
      "m shape torch.Size([512]) 439\n",
      "684\n",
      "[445, 836]\n",
      "m shape torch.Size([512]) 445\n",
      "1403\n",
      "[447, 1176]\n",
      "m shape torch.Size([512]) 447\n",
      "1403\n",
      "[448, 1349]\n",
      "m shape torch.Size([512]) 448\n",
      "1404\n",
      "[452, 570]\n",
      "m shape torch.Size([512]) 452\n",
      "1128\n",
      "[453, 575]\n",
      "m shape torch.Size([512]) 453\n",
      "1128\n",
      "[454, 658]\n",
      "m shape torch.Size([512]) 454\n",
      "1128\n",
      "[455, 721]\n",
      "m shape torch.Size([512]) 455\n",
      "1129\n",
      "[456, 639]\n",
      "m shape torch.Size([512]) 456\n",
      "1129\n",
      "[457, 774]\n",
      "m shape torch.Size([512]) 457\n",
      "1128\n",
      "[458, 785]\n",
      "m shape torch.Size([512]) 458\n",
      "1128\n",
      "[459, 814]\n",
      "m shape torch.Size([512]) 459\n",
      "1128\n",
      "[460, 817]\n",
      "m shape torch.Size([512]) 460\n",
      "1128\n",
      "[461, 877]\n",
      "m shape torch.Size([512]) 461\n",
      "1129\n",
      "[462, 886]\n",
      "m shape torch.Size([512]) 462\n",
      "1128\n",
      "[463, 1001]\n",
      "m shape torch.Size([512]) 463\n",
      "1128\n",
      "[464, 1045]\n",
      "m shape torch.Size([512]) 464\n",
      "1128\n",
      "[465, 1096]\n",
      "m shape torch.Size([512]) 465\n",
      "1128\n",
      "[466, 1129]\n",
      "m shape torch.Size([512]) 466\n",
      "1128\n",
      "[468, 1263]\n",
      "m shape torch.Size([512]) 468\n",
      "1128\n",
      "[469, 1156]\n",
      "m shape torch.Size([512]) 469\n",
      "1128\n",
      "[470, 1359]\n",
      "m shape torch.Size([512]) 470\n",
      "1128\n",
      "[472, 1571]\n",
      "m shape torch.Size([512]) 472\n",
      "1128\n",
      "[473, 626]\n",
      "m shape torch.Size([512]) 473\n",
      "1128\n",
      "[474, 887]\n",
      "m shape torch.Size([512]) 474\n",
      "1128\n",
      "[475, 1009]\n",
      "m shape torch.Size([512]) 475\n",
      "1128\n",
      "[476, 1042]\n",
      "m shape torch.Size([512]) 476\n",
      "1128\n",
      "[477, 1327]\n",
      "m shape torch.Size([512]) 477\n",
      "1128\n",
      "[478, 1235]\n",
      "m shape torch.Size([512]) 478\n",
      "1128\n",
      "[486, 1001]\n",
      "m shape torch.Size([512]) 486\n",
      "2011\n",
      "[487, 1089]\n",
      "m shape torch.Size([512]) 487\n",
      "2011\n",
      "[488, 1122]\n",
      "m shape torch.Size([512]) 488\n",
      "2011\n",
      "[489, 1137]\n",
      "m shape torch.Size([512]) 489\n",
      "2011\n",
      "[490, 1532]\n",
      "m shape torch.Size([512]) 490\n",
      "2011\n",
      "[491, 1544]\n",
      "m shape torch.Size([512]) 491\n",
      "2011\n",
      "[497, 593]\n",
      "m shape torch.Size([512]) 497\n",
      "2011\n",
      "[498, 880]\n",
      "m shape torch.Size([512]) 498\n",
      "2011\n",
      "[499, 962]\n",
      "m shape torch.Size([512]) 499\n",
      "2011\n",
      "[500, 1054]\n",
      "m shape torch.Size([512]) 500\n",
      "2011\n",
      "[501, 1123]\n",
      "m shape torch.Size([512]) 501\n",
      "2011\n",
      "[502, 1128]\n",
      "m shape torch.Size([512]) 502\n",
      "2011\n",
      "[503, 1327]\n",
      "m shape torch.Size([512]) 503\n",
      "2011\n",
      "[504, 1401]\n",
      "m shape torch.Size([512]) 504\n",
      "2011\n",
      "[505, 2018]\n",
      "m shape torch.Size([512]) 505\n",
      "2011\n",
      "[506, 2081]\n",
      "m shape torch.Size([512]) 506\n",
      "2011\n",
      "[507, 2137]\n",
      "m shape torch.Size([512]) 507\n",
      "2011\n",
      "[509, 2104]\n",
      "m shape torch.Size([512]) 509\n",
      "2012\n",
      "[511, 597]\n",
      "m shape torch.Size([512]) 511\n",
      "4971\n",
      "[512, 833]\n",
      "m shape torch.Size([512]) 512\n",
      "4971\n",
      "[513, 1045]\n",
      "m shape torch.Size([512]) 513\n",
      "4972\n",
      "[514, 1101]\n",
      "m shape torch.Size([512]) 514\n",
      "4971\n",
      "[515, 1120]\n",
      "m shape torch.Size([512]) 515\n",
      "4971\n",
      "[516, 1214]\n",
      "m shape torch.Size([512]) 516\n",
      "4972\n",
      "[517, 1227]\n",
      "m shape torch.Size([512]) 517\n",
      "4971\n",
      "[518, 1376]\n",
      "m shape torch.Size([512]) 518\n",
      "4971\n",
      "[519, 1711]\n",
      "m shape torch.Size([512]) 519\n",
      "4971\n",
      "[520, 1965]\n",
      "m shape torch.Size([512]) 520\n",
      "4971\n",
      "[521, 2023]\n",
      "m shape torch.Size([512]) 521\n",
      "4971\n",
      "[522, 2047]\n",
      "m shape torch.Size([512]) 522\n",
      "4972\n",
      "[523, 2085]\n",
      "m shape torch.Size([512]) 523\n",
      "4971\n",
      "[524, 2129]\n",
      "m shape torch.Size([512]) 524\n",
      "4972\n",
      "[525, 2146]\n",
      "m shape torch.Size([512]) 525\n",
      "4971\n",
      "[526, 2178]\n",
      "m shape torch.Size([512]) 526\n",
      "4972\n",
      "[527, 2277]\n",
      "m shape torch.Size([512]) 527\n",
      "4971\n",
      "[528, 2402]\n",
      "m shape torch.Size([512]) 528\n",
      "4971\n",
      "[529, 2457]\n",
      "m shape torch.Size([512]) 529\n",
      "4972\n",
      "[530, 2578]\n",
      "m shape torch.Size([512]) 530\n",
      "4972\n",
      "[531, 2634]\n",
      "m shape torch.Size([512]) 531\n",
      "4972\n",
      "[532, 2678]\n",
      "m shape torch.Size([512]) 532\n",
      "4972\n",
      "[533, 2678]\n",
      "m shape torch.Size([512]) 533\n",
      "4972\n",
      "[534, 2688]\n",
      "m shape torch.Size([512]) 534\n",
      "4972\n",
      "[535, 2714]\n",
      "m shape torch.Size([512]) 535\n",
      "4972\n",
      "[536, 2734]\n",
      "m shape torch.Size([512]) 536\n",
      "4972\n",
      "[537, 2796]\n",
      "m shape torch.Size([512]) 537\n",
      "4972\n",
      "[538, 2911]\n",
      "m shape torch.Size([512]) 538\n",
      "4972\n",
      "[539, 2975]\n",
      "m shape torch.Size([512]) 539\n",
      "4971\n",
      "[540, 3065]\n",
      "m shape torch.Size([512]) 540\n",
      "4971\n",
      "[541, 3082]\n",
      "m shape torch.Size([512]) 541\n",
      "4972\n",
      "[542, 3142]\n",
      "m shape torch.Size([512]) 542\n",
      "4972\n",
      "[543, 3207]\n",
      "m shape torch.Size([512]) 543\n",
      "4972\n",
      "[544, 3314]\n",
      "m shape torch.Size([512]) 544\n",
      "4971\n",
      "[545, 3604]\n",
      "m shape torch.Size([512]) 545\n",
      "4972\n",
      "[546, 3671]\n",
      "m shape torch.Size([512]) 546\n",
      "4972\n",
      "[547, 3745]\n",
      "m shape torch.Size([512]) 547\n",
      "4972\n",
      "[548, 3886]\n",
      "m shape torch.Size([512]) 548\n",
      "4972\n",
      "[549, 3994]\n",
      "m shape torch.Size([512]) 549\n",
      "4971\n",
      "[550, 4072]\n",
      "m shape torch.Size([512]) 550\n",
      "4972\n",
      "[551, 4575]\n",
      "m shape torch.Size([512]) 551\n",
      "4972\n",
      "[552, 4690]\n",
      "m shape torch.Size([512]) 552\n",
      "4971\n",
      "[553, 4737]\n",
      "m shape torch.Size([512]) 553\n",
      "4971\n",
      "[554, 4767]\n",
      "m shape torch.Size([512]) 554\n",
      "4972\n",
      "[555, 5388]\n",
      "m shape torch.Size([512]) 555\n",
      "4972\n",
      "[556, 5455]\n",
      "m shape torch.Size([512]) 556\n",
      "4972\n",
      "[557, 5569]\n",
      "m shape torch.Size([512]) 557\n",
      "4972\n",
      "[558, 5651]\n",
      "m shape torch.Size([512]) 558\n",
      "4971\n",
      "[559, 5681]\n",
      "m shape torch.Size([512]) 559\n",
      "4972\n",
      "[560, 6282]\n",
      "m shape torch.Size([512]) 560\n",
      "4971\n",
      "[561, 6305]\n",
      "m shape torch.Size([512]) 561\n",
      "4971\n",
      "[562, 6326]\n",
      "m shape torch.Size([512]) 562\n",
      "4971\n",
      "[563, 530]\n",
      "m shape torch.Size([512]) 563\n",
      "1379\n",
      "[564, 1206]\n",
      "m shape torch.Size([512]) 564\n",
      "1379\n",
      "[565, 1230]\n",
      "m shape torch.Size([512]) 565\n",
      "1380\n",
      "[566, 1217]\n",
      "m shape torch.Size([512]) 566\n",
      "1379\n",
      "[568, 537]\n",
      "m shape torch.Size([512]) 568\n",
      "1379\n",
      "[569, 1032]\n",
      "m shape torch.Size([512]) 569\n",
      "1380\n",
      "[570, 1032]\n",
      "m shape torch.Size([512]) 570\n",
      "1380\n",
      "[571, 530]\n",
      "m shape torch.Size([512]) 571\n",
      "1379\n",
      "[572, 1042]\n",
      "m shape torch.Size([512]) 572\n",
      "1379\n",
      "[573, 1177]\n",
      "m shape torch.Size([512]) 573\n",
      "1379\n",
      "[574, 1198]\n",
      "m shape torch.Size([512]) 574\n",
      "1379\n",
      "[575, 1389]\n",
      "m shape torch.Size([512]) 575\n",
      "1379\n",
      "[577, 1230]\n",
      "m shape torch.Size([512]) 577\n",
      "1380\n",
      "[585, 769]\n",
      "m shape torch.Size([512]) 585\n",
      "850\n",
      "[586, 776]\n",
      "m shape torch.Size([512]) 586\n",
      "850\n",
      "[587, 794]\n",
      "m shape torch.Size([512]) 587\n",
      "850\n",
      "[609, 598]\n",
      "m shape torch.Size([512]) 609\n",
      "1886\n",
      "[610, 660]\n",
      "m shape torch.Size([512]) 610\n",
      "1886\n",
      "[611, 699]\n",
      "m shape torch.Size([512]) 611\n",
      "1886\n",
      "[612, 815]\n",
      "m shape torch.Size([512]) 612\n",
      "1887\n",
      "[613, 936]\n",
      "m shape torch.Size([512]) 613\n",
      "1886\n",
      "[614, 984]\n",
      "m shape torch.Size([512]) 614\n",
      "1886\n",
      "[615, 1133]\n",
      "m shape torch.Size([512]) 615\n",
      "1886\n",
      "[616, 1316]\n",
      "m shape torch.Size([512]) 616\n",
      "1886\n",
      "[617, 1646]\n",
      "m shape torch.Size([512]) 617\n",
      "1886\n",
      "[618, 1677]\n",
      "m shape torch.Size([512]) 618\n",
      "1886\n",
      "[619, 1780]\n",
      "m shape torch.Size([512]) 619\n",
      "1886\n",
      "[620, 1803]\n",
      "m shape torch.Size([512]) 620\n",
      "1887\n",
      "[621, 1808]\n",
      "m shape torch.Size([512]) 621\n",
      "1887\n",
      "[622, 1846]\n",
      "m shape torch.Size([512]) 622\n",
      "1886\n",
      "[623, 2132]\n",
      "m shape torch.Size([512]) 623\n",
      "1886\n",
      "[648, 776]\n",
      "m shape torch.Size([512]) 648\n",
      "1098\n",
      "[649, 786]\n",
      "m shape torch.Size([512]) 649\n",
      "1098\n",
      "[655, 519]\n",
      "m shape torch.Size([512]) 655\n",
      "1098\n",
      "[656, 522]\n",
      "m shape torch.Size([512]) 656\n",
      "1098\n",
      "[657, 528]\n",
      "m shape torch.Size([512]) 657\n",
      "1098\n",
      "[660, 662]\n",
      "m shape torch.Size([512]) 660\n",
      "1098\n",
      "[661, 724]\n",
      "m shape torch.Size([512]) 661\n",
      "1098\n",
      "[662, 1192]\n",
      "m shape torch.Size([512]) 662\n",
      "1098\n",
      "[663, 1218]\n",
      "m shape torch.Size([512]) 663\n",
      "1098\n",
      "[664, 1133]\n",
      "m shape torch.Size([512]) 664\n",
      "1098\n",
      "[674, 520]\n",
      "m shape torch.Size([512]) 674\n",
      "595\n",
      "[675, 527]\n",
      "m shape torch.Size([512]) 675\n",
      "595\n",
      "[676, 733]\n",
      "m shape torch.Size([512]) 676\n",
      "595\n",
      "[677, 548]\n",
      "m shape torch.Size([512]) 677\n",
      "616\n",
      "[678, 548]\n",
      "m shape torch.Size([512]) 678\n",
      "616\n",
      "[679, 568]\n",
      "m shape torch.Size([512]) 679\n",
      "616\n",
      "[680, 583]\n",
      "m shape torch.Size([512]) 680\n",
      "617\n",
      "[681, 587]\n",
      "m shape torch.Size([512]) 681\n",
      "616\n",
      "[682, 611]\n",
      "m shape torch.Size([512]) 682\n",
      "616\n",
      "[683, 636]\n",
      "m shape torch.Size([512]) 683\n",
      "617\n",
      "[684, 683]\n",
      "m shape torch.Size([512]) 684\n",
      "616\n",
      "[685, 685]\n",
      "m shape torch.Size([512]) 685\n",
      "617\n",
      "[689, 1553]\n",
      "m shape torch.Size([512]) 689\n",
      "5611\n",
      "[690, 1574]\n",
      "m shape torch.Size([512]) 690\n",
      "5611\n",
      "[691, 1580]\n",
      "m shape torch.Size([512]) 691\n",
      "5611\n",
      "[692, 1592]\n",
      "m shape torch.Size([512]) 692\n",
      "5611\n",
      "[693, 1607]\n",
      "m shape torch.Size([512]) 693\n",
      "5611\n",
      "[694, 1693]\n",
      "m shape torch.Size([512]) 694\n",
      "5611\n",
      "[695, 1696]\n",
      "m shape torch.Size([512]) 695\n",
      "5611\n",
      "[696, 1758]\n",
      "m shape torch.Size([512]) 696\n",
      "5611\n",
      "[697, 1765]\n",
      "m shape torch.Size([512]) 697\n",
      "5611\n",
      "[698, 1792]\n",
      "m shape torch.Size([512]) 698\n",
      "5612\n",
      "[699, 1816]\n",
      "m shape torch.Size([512]) 699\n",
      "5611\n",
      "[700, 1818]\n",
      "m shape torch.Size([512]) 700\n",
      "5611\n",
      "[701, 1837]\n",
      "m shape torch.Size([512]) 701\n",
      "5611\n",
      "[702, 1919]\n",
      "m shape torch.Size([512]) 702\n",
      "5611\n",
      "[703, 1964]\n",
      "m shape torch.Size([512]) 703\n",
      "5611\n",
      "[704, 2547]\n",
      "m shape torch.Size([512]) 704\n",
      "5611\n",
      "[705, 2941]\n",
      "m shape torch.Size([512]) 705\n",
      "5612\n",
      "[706, 3004]\n",
      "m shape torch.Size([512]) 706\n",
      "5611\n",
      "[707, 3109]\n",
      "m shape torch.Size([512]) 707\n",
      "5611\n",
      "[708, 3178]\n",
      "m shape torch.Size([512]) 708\n",
      "5611\n",
      "[709, 3180]\n",
      "m shape torch.Size([512]) 709\n",
      "5611\n",
      "[710, 3187]\n",
      "m shape torch.Size([512]) 710\n",
      "5611\n",
      "[711, 3387]\n",
      "m shape torch.Size([512]) 711\n",
      "5612\n",
      "[712, 4299]\n",
      "m shape torch.Size([512]) 712\n",
      "5611\n",
      "[713, 4541]\n",
      "m shape torch.Size([512]) 713\n",
      "5611\n",
      "[714, 6079]\n",
      "m shape torch.Size([512]) 714\n",
      "5611\n",
      "[715, 6444]\n",
      "m shape torch.Size([512]) 715\n",
      "5612\n",
      "[716, 6467]\n",
      "m shape torch.Size([512]) 716\n",
      "5612\n",
      "[717, 6501]\n",
      "m shape torch.Size([512]) 717\n",
      "5611\n",
      "[718, 6972]\n",
      "m shape torch.Size([512]) 718\n",
      "5611\n",
      "[719, 7424]\n",
      "m shape torch.Size([512]) 719\n",
      "5611\n",
      "[720, 7425]\n",
      "m shape torch.Size([512]) 720\n",
      "5611\n",
      "[721, 7549]\n",
      "m shape torch.Size([512]) 721\n",
      "5611\n",
      "[722, 7549]\n",
      "m shape torch.Size([512]) 722\n",
      "5611\n",
      "[723, 7599]\n",
      "m shape torch.Size([512]) 723\n",
      "5611\n",
      "[724, 7605]\n",
      "m shape torch.Size([512]) 724\n",
      "5611\n",
      "[725, 7605]\n",
      "m shape torch.Size([512]) 725\n",
      "5611\n",
      "[738, 709]\n",
      "m shape torch.Size([512]) 738\n",
      "682\n",
      "[739, 710]\n",
      "m shape torch.Size([512]) 739\n",
      "682\n",
      "[745, 753]\n",
      "m shape torch.Size([512]) 745\n",
      "7633\n",
      "[746, 801]\n",
      "m shape torch.Size([512]) 746\n",
      "7633\n",
      "[747, 839]\n",
      "m shape torch.Size([512]) 747\n",
      "7633\n",
      "[761, 516]\n",
      "m shape torch.Size([512]) 761\n",
      "646\n",
      "[762, 533]\n",
      "m shape torch.Size([512]) 762\n",
      "646\n",
      "[763, 566]\n",
      "m shape torch.Size([512]) 763\n",
      "646\n",
      "[764, 566]\n",
      "m shape torch.Size([512]) 764\n",
      "646\n",
      "[765, 616]\n",
      "m shape torch.Size([512]) 765\n",
      "647\n",
      "[766, 616]\n",
      "m shape torch.Size([512]) 766\n",
      "647\n",
      "[767, 714]\n",
      "m shape torch.Size([512]) 767\n",
      "646\n",
      "[768, 645]\n",
      "m shape torch.Size([512]) 768\n",
      "646\n",
      "[777, 513]\n",
      "m shape torch.Size([512]) 777\n",
      "564\n",
      "[782, 607]\n",
      "m shape torch.Size([512]) 782\n",
      "565\n",
      "[801, 586]\n",
      "m shape torch.Size([512]) 801\n",
      "564\n",
      "[802, 551]\n",
      "m shape torch.Size([512]) 802\n",
      "564\n",
      "[803, 551]\n",
      "m shape torch.Size([512]) 803\n",
      "564\n",
      "[804, 586]\n",
      "m shape torch.Size([512]) 804\n",
      "564\n",
      "[829, 795]\n",
      "m shape torch.Size([512]) 829\n",
      "849\n",
      "[844, 523]\n",
      "m shape torch.Size([512]) 844\n",
      "1055\n",
      "[845, 529]\n",
      "m shape torch.Size([512]) 845\n",
      "1055\n",
      "[846, 517]\n",
      "m shape torch.Size([512]) 846\n",
      "1055\n"
     ]
    }
   ],
   "source": [
    "overflow_idx=[]\n",
    "[x[1] for x in loc_mentions]\n",
    "for i,j in enumerate(loc_mentions):\n",
    "    if j[1]>=512:\n",
    "        overflow_idx.append([i, j[1] ])\n",
    "        \n",
    "        \n",
    "        \n",
    "idx = [x[0] for x in overflow_idx]\n",
    "\n",
    "# res=list(np.array(bert_sentences)[idx])\n",
    "# len(res)\n",
    "\n",
    "m_new  = []\n",
    "for i, j in enumerate(overflow_idx):\n",
    "    print(j )\n",
    "    bert_sent = bert_sentences[j[0] ]\n",
    "    \n",
    "    #m[j[0]] = m[j[0], j[1]-4096:j[1]] \n",
    "    print(\"m shape\",m[j[0],  j[1]-512+1:j[1]+1].shape,j[0] )\n",
    "    m_new.append(m[j[0],  j[1]-512+1:j[1]+1] )\n",
    "    print(len(bert_sent.split()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "79aba0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_new_tensor = torch.stack(m_new) \n",
    " \n",
    "#torch.nn.functional.pad(m_new_tensor, pad = (0,1), mode='constant', value=1)\n",
    "m_new_tensor = F.pad(m_new_tensor,pad =(0, m.shape[1]-m_new_tensor.shape[1]), value=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e25d4f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([418, 12006])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_new_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "513049e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m[idx] = m_new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "13fd1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = m == parallel_coref_model.module.vals[0]\n",
    "p = m == parallel_coref_model.module.vals[1]\n",
    "\n",
    "v = (k.int() + p.int()).bool()\n",
    "nz_indexes = v.nonzero()[:, 1].reshape(m.shape[0], 2)\n",
    "\n",
    "q = torch.arange(m.shape[1])\n",
    "q = q.repeat(m.shape[0], 1)\n",
    "\n",
    "msk_0 = (nz_indexes[:, 0].repeat(m.shape[1], 1).transpose(0, 1)) <= q\n",
    "msk_1 = (nz_indexes[:, 1].repeat(m.shape[1], 1).transpose(0, 1)) >= q\n",
    "\n",
    "msk_0_ar = (nz_indexes[:, 0].repeat(m.shape[1], 1).transpose(0, 1)) < q\n",
    "msk_1_ar = (nz_indexes[:, 1].repeat(m.shape[1], 1).transpose(0, 1)) > q\n",
    "\n",
    "attention_mask_g = msk_0.int() * msk_1.int()\n",
    "\n",
    "input_ids = input_ids[:, :512]\n",
    "# attention_mask = attention_mask[:, :4096]\n",
    "#attention_mask[:, 0] = 2\n",
    "#attention_mask[attention_mask_g == 1] = 2\n",
    "attention_mask = attention_mask[:, :512]\n",
    "arg1 = msk_0_ar.int() * msk_1_ar.int()\n",
    "arg_new1 = msk_0.int() * msk_1.int()\n",
    "arg_new1 = arg_new1[:,:512]\n",
    "arg1 = arg1[:, :512]\n",
    "arg1 = arg1.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4fa85f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating : 100%|██████████| 6/6 [00:00<00:00, 32305.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 512])\n",
      "0\n",
      "torch.Size([150, 512])\n",
      "150\n",
      "torch.Size([150, 512])\n",
      "300\n",
      "torch.Size([150, 512])\n",
      "450\n",
      "torch.Size([150, 512])\n",
      "600\n",
      "torch.Size([97, 512])\n",
      "750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 150\n",
    "batch = [input_ids[x] for x in range(i, min(i + batch_size, len(bert_sentences)))]\n",
    "for i in tqdm(range(0, len(bert_sentences), batch_size), desc=\"generating \"):\n",
    "    print(input_ids[i:i+batch_size].shape)\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2554aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating : 100%|██████████| 17/17 [00:03<00:00,  4.48it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vectors = []\n",
    "batch_size = 50\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(bert_sentences), batch_size), desc=\"generating \"):\n",
    "        #men_vectors = parallel_coref_model(input_ids[i:i+batch_size], attention_mask[i:i+batch_size], arg1[i:i+batch_size]).detach().cpu().numpy()\n",
    "        men_vectors = parallel_coref_model(input_ids[i:i+batch_size], attention_mask[i:i+batch_size], arg1[i:i+batch_size]) \n",
    "        all_vectors.extend(men_vectors)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8b091c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "765126d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_iterator = zip(men_ids, all_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7ea76319",
   "metadata": {},
   "outputs": [],
   "source": [
    "corefbert_eve_vec_map_longdoc = dict(zip_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7dcb2725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3762e+00,  3.9003e-01,  2.7317e-01,  1.1907e-01,  6.9881e-01,\n",
       "         2.3653e-02, -6.3391e-02,  4.1318e-02,  1.9541e-01, -3.4547e-01,\n",
       "         2.3975e-01, -4.5153e-01, -9.4092e-01,  1.5608e-01, -6.5492e-01,\n",
       "        -1.3030e-01,  4.8318e-01, -1.0549e+00, -4.3398e-02,  1.6455e-01,\n",
       "        -7.9633e-01,  2.2138e-01, -6.0868e-01,  7.2251e-01,  7.7709e-01,\n",
       "        -1.1637e+00,  3.2054e-01,  6.4990e-01,  3.4371e-01, -7.3551e-01,\n",
       "         1.3439e+00, -4.0253e-01, -6.9410e-02,  7.6835e-01, -9.4313e-01,\n",
       "        -4.1650e-01,  7.8101e-01, -4.5216e-01, -1.0445e+00, -2.5581e-01,\n",
       "         5.5291e-01, -6.0835e-01, -9.3392e-01,  1.7786e+00, -1.1582e-01,\n",
       "        -9.8942e-02, -2.1288e-02,  5.5209e-01, -9.4017e-01, -5.9559e-01,\n",
       "         1.7318e-01,  1.9765e-01,  1.3828e-01,  1.9807e-01, -6.4563e-02,\n",
       "        -4.7534e-01, -8.1373e-01, -7.4151e-01, -4.3523e-01,  1.3022e+00,\n",
       "        -1.1737e-01, -6.5041e-01, -1.1155e-01, -3.5110e-01, -2.7437e-01,\n",
       "         1.2891e+00, -6.7272e-01, -2.2472e-01,  4.4265e-01, -3.1535e-01,\n",
       "         6.8704e-01, -7.5392e-01, -9.1098e-01,  6.9506e-01,  3.0971e-01,\n",
       "         2.1239e-03,  1.0982e-01, -1.0525e+00,  7.9450e-01,  8.0259e-01,\n",
       "         5.6325e-01,  3.7591e-01,  1.6281e-01,  1.3629e+00, -3.8615e-02,\n",
       "         8.7134e-01, -2.8095e-01, -1.5805e-01,  1.0738e-01,  7.0124e-01,\n",
       "         9.5706e-02,  7.2318e-01,  1.2347e+00,  6.1811e-01,  1.0049e+00,\n",
       "         5.1546e-01,  1.2694e-01, -3.0323e-01, -5.6787e-01, -1.5056e-01,\n",
       "        -3.6992e-01, -1.5023e+00,  3.7745e-01,  1.3552e+00,  8.3492e-01,\n",
       "        -1.1198e+00,  1.3548e+00, -1.9486e+00, -1.8498e-02,  6.7759e-01,\n",
       "        -5.3683e-01, -1.4131e-01, -6.1149e-01, -1.6107e-01,  7.4980e-01,\n",
       "        -1.3432e+00, -1.6819e-01,  5.5563e-01,  8.4604e-02, -1.8783e-02,\n",
       "         6.9246e-02, -3.4386e-01,  4.9155e-02,  6.3734e-02,  2.3811e-01,\n",
       "        -6.1604e-02, -4.5342e-01, -5.2842e-01,  4.4952e-01,  4.3649e-01,\n",
       "        -8.5048e-01, -4.2564e-01,  4.4836e-01,  2.2786e-01, -6.3877e-01,\n",
       "        -3.8607e-01,  8.6687e-01, -1.1125e+00, -1.2699e+00, -2.5423e-01,\n",
       "        -6.3296e-01,  7.4424e-01,  1.7556e-02,  1.2200e+00, -1.7822e-01,\n",
       "         1.0296e+00, -2.3945e-02,  3.5563e-01,  1.1738e+00,  9.5573e-01,\n",
       "        -1.2190e+00, -4.1873e-01,  8.0581e-01,  1.3805e+00, -6.6838e-01,\n",
       "        -1.2488e+00,  6.0392e-01,  3.7731e-01, -4.0741e-01,  1.5753e+00,\n",
       "        -5.3775e-01, -7.1661e-01,  5.5557e-01,  8.1514e-01, -5.2722e-01,\n",
       "        -1.0353e-01, -1.5138e-01,  2.7270e-01,  2.9311e-01, -7.5412e-01,\n",
       "         6.0702e-04, -3.5455e-01, -2.4411e-01, -5.5773e-01,  7.0310e-01,\n",
       "        -1.2374e+00,  6.7943e-02,  7.9581e-01, -1.1514e+00,  6.5121e-01,\n",
       "        -1.6966e-01, -2.5817e-01,  1.1098e+00,  1.5367e+00,  2.8807e-01,\n",
       "        -9.9309e-01, -4.4090e-01, -4.6432e-01,  1.1017e+00, -1.0926e-01,\n",
       "        -4.5452e-01,  4.8514e-01,  2.5028e-01, -6.2283e-01,  5.4366e-03,\n",
       "        -7.1969e-01, -3.7918e-01,  4.4349e-03,  9.2201e-01,  9.4410e-01,\n",
       "         4.6340e-01,  3.9766e-02,  5.4986e-01, -1.1574e+00,  1.0763e-02,\n",
       "        -2.3654e-01,  1.8916e-01, -1.6303e-01,  1.1415e-01, -9.1459e-01,\n",
       "        -8.2066e-02,  5.5417e-01, -9.8248e-01, -9.5191e-01, -7.3212e-01,\n",
       "        -9.2782e-01,  1.2879e-01,  1.0168e+00, -4.7173e-01, -1.9455e-01,\n",
       "        -1.0188e+00, -4.2859e-01, -2.2773e-01,  2.5584e-01,  6.3350e-02,\n",
       "         7.7892e-02,  7.7838e-01,  1.4957e+00,  7.1185e-01, -1.0538e+00,\n",
       "        -4.7751e-01, -2.7241e-01,  1.1939e-01, -5.7649e-01, -9.5926e-01,\n",
       "         5.4623e-01, -1.8842e-01, -4.2030e-01, -2.1296e-01,  9.1808e-01,\n",
       "        -1.2692e+00, -3.3530e-01,  8.1148e-01, -7.2803e-01, -7.7485e-01,\n",
       "         4.2768e-01, -9.3840e-01,  3.1160e-01,  4.9136e-01,  1.1433e+00,\n",
       "        -6.0452e-02, -5.0561e-01, -9.2220e-01, -4.8287e-01,  1.6798e+00,\n",
       "         6.8627e-01, -1.2588e-01,  4.5948e-01, -8.3900e-01, -9.2891e-01,\n",
       "         5.0095e-01, -4.6211e-01,  3.2437e-01, -2.1110e-01, -6.8538e-01,\n",
       "        -9.8483e-01,  2.1606e-01,  1.5443e+00, -8.9858e-01, -8.2466e-01,\n",
       "         3.3318e-01,  1.1549e+00, -7.4451e-01,  1.1978e-01, -1.0128e+00,\n",
       "         4.0274e-01,  1.0473e-01, -7.2611e-01, -2.8895e-01, -5.6373e-01,\n",
       "        -7.1569e-02, -2.3619e-01,  2.1885e-01,  1.8351e+00,  1.2107e-01,\n",
       "        -5.1570e-01,  2.3163e-01,  9.5666e-01, -3.0348e-01,  1.7754e+00,\n",
       "         7.8180e-01,  9.1173e-01, -4.9815e-01, -5.5227e-01, -8.7649e-01,\n",
       "        -1.6552e-01, -6.3149e-01,  8.8982e-02, -5.0464e-01,  5.9283e-01,\n",
       "        -1.5296e-01,  2.3107e-01, -8.5393e-02,  2.6999e-01,  8.9530e-03,\n",
       "        -4.7912e-01, -1.3095e+00, -5.9710e-02,  5.5122e-01,  1.3401e+00,\n",
       "         1.3748e-01, -5.5379e-01,  1.6908e+00, -2.1896e-01,  3.8425e-01,\n",
       "        -1.0835e-01, -8.2482e-01,  9.3566e-01, -2.4495e-01,  4.7542e-02,\n",
       "         8.1056e-01, -2.6317e-01,  1.9771e-01, -6.2010e-01,  7.0429e-02,\n",
       "         1.9272e-01, -2.0207e-01, -5.7134e-01, -7.4893e-02, -2.5509e-01,\n",
       "        -1.1276e+00,  4.4602e-01,  1.0591e-01, -2.4104e-01,  1.1044e+00,\n",
       "         1.4674e+00,  9.3962e-01,  3.7066e-01, -1.3455e+00, -4.3126e-01,\n",
       "        -9.3318e-01, -7.5086e-02,  7.2559e-01, -9.2203e-02, -8.2432e-02,\n",
       "        -2.0073e-01, -4.4586e-01, -4.4172e-02,  7.7580e-01,  7.6850e-01,\n",
       "         7.7391e-01, -1.8754e-02,  3.5056e-03,  1.1061e+00, -1.8623e+00,\n",
       "        -1.2805e-01, -7.5122e-01,  1.6432e+00,  5.6028e-01,  6.9604e-01,\n",
       "        -5.5200e-01, -3.9662e-02,  2.2746e-01,  2.9484e-01, -4.6414e-01,\n",
       "        -4.4981e-01,  1.2698e+00,  1.4485e+00,  1.5734e-01,  2.2713e-01,\n",
       "         1.0336e-01, -2.6291e-02, -8.6890e-01,  9.3181e-01, -4.6730e-01,\n",
       "         3.9943e-01,  8.4256e-01, -9.9446e-01, -9.3471e-02, -4.4637e-01,\n",
       "        -5.4819e-02,  7.2488e-01, -3.9256e-01,  2.9528e-02, -5.9485e-02,\n",
       "        -1.9783e-01,  8.7674e-01,  5.3967e-01,  1.1246e-01, -2.6244e-01,\n",
       "        -4.1815e-01, -1.1357e+00, -1.2499e-01,  1.3585e-02, -9.0995e-01,\n",
       "        -1.1846e+00,  1.8384e-01,  2.2418e-01,  1.8166e-01,  1.7226e-01,\n",
       "        -1.8392e-01, -8.7867e-01, -4.6868e-01,  9.2928e-01,  1.0860e-01,\n",
       "         8.7793e-01,  8.4153e-01, -4.1953e-01, -7.6279e-01, -5.7229e-02,\n",
       "         5.0442e-01, -2.8834e-01, -8.1043e-02,  1.2698e-01, -6.1080e-01,\n",
       "        -3.1166e-01,  1.0762e+00,  6.4500e-01,  1.4717e-01, -4.7120e-01,\n",
       "         1.0513e-01,  2.1229e-01,  1.0556e+00,  8.6419e-01, -1.3534e-01,\n",
       "        -6.7281e-01,  4.8660e-01, -1.2540e+00,  2.2938e-01, -9.3200e-02,\n",
       "         5.3654e-01,  7.9860e-01,  4.5640e-01,  1.2729e-01,  9.1500e-01,\n",
       "        -1.8484e-03, -3.3570e-01,  1.0459e-01, -5.5131e-01, -7.2419e-01,\n",
       "         1.1327e+00, -8.3535e-01,  6.1070e-03, -9.2460e-01, -6.2620e-01,\n",
       "         2.0201e-02,  1.6306e-01, -6.3139e-01,  2.0069e-01,  6.3786e-01,\n",
       "         3.4386e-01,  3.8137e-02,  5.7850e-01,  6.4765e-01,  1.5703e-02,\n",
       "        -2.8944e-02, -6.2492e-01, -2.8701e-01, -1.1467e+00,  3.8767e-01,\n",
       "         1.8939e-01, -6.7950e-01,  2.0303e-01, -2.0288e-01, -6.9266e-01,\n",
       "         7.1244e-01, -6.4619e-03,  2.1427e-01, -7.6470e-01, -7.3026e-01,\n",
       "         1.6690e-02,  3.5842e-02,  1.0822e+00, -2.8208e-02, -7.3309e-01,\n",
       "        -7.2195e-02, -2.1659e+00,  2.1059e-01,  2.2858e-02,  7.5622e-02,\n",
       "        -4.0045e-01, -8.3296e-01, -2.1127e-01,  5.1223e-01,  1.1319e+00,\n",
       "        -3.4555e-01, -9.7547e-01,  3.6821e-01,  4.9800e-02,  1.5409e-02,\n",
       "        -1.0967e-01,  2.5634e-01, -2.8575e-01, -1.7290e-01,  9.4491e-02,\n",
       "         4.9102e-01,  4.6313e-01,  1.6720e-01, -7.2163e-01,  1.7435e-01,\n",
       "        -4.2039e-01, -1.3712e+00, -4.6757e-01,  6.0972e-01, -3.6376e-01,\n",
       "        -3.2112e-01,  4.2119e-01,  1.1703e-01,  3.2167e-01, -2.9613e-01,\n",
       "         3.7038e-02, -7.8046e-01,  7.6638e-01, -9.5361e-01,  8.6875e-02,\n",
       "         4.3819e-02, -1.0852e+00,  3.5828e-01,  4.5618e-01, -8.0254e-02,\n",
       "         2.6312e-01,  2.6313e-01,  4.4170e-01,  8.8171e-01,  9.4574e-01,\n",
       "        -4.0735e-01,  7.2894e-01, -5.2162e-01, -1.1407e+00, -5.8004e-02,\n",
       "        -1.6139e+00, -7.3880e-02, -1.2156e+00, -1.0418e+00,  8.7419e-02,\n",
       "        -7.4678e-02, -9.8645e-01, -7.5537e-02, -4.4880e-02,  7.9800e-01,\n",
       "        -8.0364e-01,  6.8619e-01,  1.4616e-01, -1.5393e-01,  4.7520e-01,\n",
       "         4.8243e-01,  3.4672e-01, -2.5523e-01, -1.4196e+00,  1.1578e+00,\n",
       "         1.0369e+00, -6.0829e-01,  7.2266e-01,  2.1425e-01, -9.6480e-01,\n",
       "        -1.7787e-01, -1.4432e-02, -6.4537e+00,  7.3155e-01, -1.9619e-01,\n",
       "         2.8320e-01, -3.3559e-01, -1.2411e+00,  4.2724e-01, -7.5737e-02,\n",
       "         2.5653e-01,  9.7616e-01, -7.7857e-01, -6.0897e-01,  1.1355e-01,\n",
       "         4.1106e-01, -1.9617e-01, -5.5140e-01, -6.3458e-01, -1.5676e+00,\n",
       "        -2.7327e-01,  1.6346e-02, -4.9038e-01,  8.0238e-01,  1.3094e-01,\n",
       "        -9.6138e-02,  9.2953e-01,  7.6782e-01, -5.0328e-01,  4.5625e-02,\n",
       "         2.2517e-01, -7.3315e-01,  3.3701e-01,  2.5196e-01,  1.0404e-01,\n",
       "         4.8963e-01,  2.4792e-01, -5.7500e-01, -3.8884e-01,  6.2207e-01,\n",
       "        -7.7717e-01,  6.9190e-01,  7.8739e-01,  3.2032e-01,  1.4887e+00,\n",
       "        -8.0585e-01,  8.3485e-01,  3.2493e-01, -1.1257e+00, -1.5658e+00,\n",
       "         3.5064e-01, -3.6966e-01, -3.5241e-01, -4.9414e-02,  1.2157e-01,\n",
       "         5.6405e-01, -4.4548e-01, -2.8395e-01, -1.5669e-01, -1.4271e-01,\n",
       "         6.4678e-01, -1.0624e+00, -5.0210e-01,  1.3728e-01,  2.0240e-02,\n",
       "         4.7330e-01, -6.8798e-01,  3.4288e-01,  1.6685e-01, -2.3157e-01,\n",
       "        -2.6151e-01,  5.6374e-01, -9.9693e-01, -8.6101e-02,  9.0982e-01,\n",
       "        -5.0112e-01, -6.1083e-03,  3.9777e-01,  1.0632e-01,  3.2013e-01,\n",
       "         3.9966e-01, -1.2440e-01, -2.0339e-01,  5.0065e-01, -1.4255e+00,\n",
       "         2.9716e-01, -3.0460e-01,  5.9952e-01, -1.1944e+00,  3.2924e-01,\n",
       "        -3.9271e-01, -7.3206e-01,  5.8932e-01,  6.3779e-02, -2.7674e-01,\n",
       "         1.0096e+00, -1.4876e+00,  8.0870e-01, -3.0886e-01, -5.7947e-01,\n",
       "        -8.3352e-02, -1.2715e-01, -1.1573e+00,  6.8404e-01,  9.3944e-01,\n",
       "         1.1810e+00,  6.0697e-01,  5.1729e-01,  2.9745e-01, -1.3648e+00,\n",
       "         1.1692e+00, -6.1429e-01, -2.3316e-01, -7.0295e-01,  2.3729e+00,\n",
       "         1.1442e+00, -2.8940e-01,  2.5312e-01, -1.1220e-01, -7.9446e-01,\n",
       "         7.1652e-01,  4.5659e-01,  2.6520e-02,  1.6247e-01, -5.4600e-01,\n",
       "         5.3632e-01,  8.3961e-01,  7.7650e-01, -8.6968e-02,  1.4624e+00,\n",
       "         6.3631e-01, -7.4380e-01,  4.9353e-01, -7.1035e-01, -1.8997e+00,\n",
       "         1.5521e+00,  1.2637e+00, -1.5573e+00,  8.5554e-01,  5.2008e-01,\n",
       "        -3.8271e-01,  6.4967e-01, -2.4432e-01,  3.6770e-01,  1.5694e-02,\n",
       "        -1.0229e+00,  7.0335e-01,  6.8074e-01, -7.8298e-02,  4.8546e-01,\n",
       "         4.2560e-02, -7.3520e-01, -3.5328e-01,  2.0767e-01, -2.8563e-01,\n",
       "        -7.6220e-01,  4.3967e-02,  4.7946e-01,  5.1761e-02,  8.7970e-01,\n",
       "        -2.2274e-01, -9.7771e-01, -6.6837e-01, -1.8302e+00,  1.2098e+00,\n",
       "         6.9557e-01, -1.3941e+00,  4.4789e-01,  3.0411e-01, -5.6726e-01,\n",
       "        -5.8812e-01,  2.8283e-01, -1.1910e+00,  4.1704e-01, -4.7091e-01,\n",
       "        -6.4025e-01, -6.5207e-02,  1.3217e-01,  1.2460e+00,  4.2769e-01,\n",
       "         3.7414e-01,  4.3411e-01, -1.5751e-01,  1.7567e+00,  2.3227e-01,\n",
       "        -4.3861e-01,  4.4300e-01,  3.4796e-01, -9.3340e-02, -7.2327e-01,\n",
       "        -3.4209e-01,  1.8262e-01, -8.9752e-01,  3.5590e-01, -1.5988e+00,\n",
       "         4.6404e-01, -1.7941e-01,  1.1461e+00,  3.7143e-01, -3.8292e-01,\n",
       "         5.4676e-02,  1.2320e-01,  2.6509e-01,  7.4034e-01,  1.3601e-02,\n",
       "         1.1231e-01, -1.4244e-01,  8.0910e-01,  5.2950e-01, -1.3217e+00,\n",
       "        -7.4815e-02,  3.0841e-01,  7.8107e-01], device='cuda:0')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corefbert_eve_vec_map_longdoc['VMIC0015MXG.000003'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "901d53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corefbert_eve_mention_map_longdoc_file = working_folder + '/corefbert_evt_vectorn_map_longbertdoc.pkl'\n",
    "#pickle.dump(corefbert_eve_vec_map_longdoc, open(corefbert_eve_mention_map_longdoc_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a94a0730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5ecaff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "corefbert_vector_map_evt_longbertdoc = pickle.load(open(corefbert_eve_mention_map_longdoc_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "16df726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # which coreference mention map\n",
    "# if men_type == 'evt':\n",
    "#     curr_mention_map = eve_mention_map\n",
    "# else:\n",
    "#     curr_mention_map = ent_mention_map\n",
    "\n",
    "curr_mention_map = eve_mention_map \n",
    "# create a single dict for all mentions\n",
    "all_mention_map = {**eve_mention_map, **ent_mention_map}\n",
    "\n",
    "# sort event mentions and make men to ind map\n",
    "curr_mentions = sorted(list(curr_mention_map.keys()), key=lambda x: curr_mention_map[x]['m_id'])\n",
    "curr_men_to_ind = {eve: i for i, eve in enumerate(curr_mentions)}\n",
    "\n",
    "# generate gold clusters key file\n",
    "curr_gold_cluster_map = [(men, all_mention_map[men]['gold_cluster']) for men in curr_mentions]\n",
    "gold_key_file = working_folder + f'/{men_type}_gold.keyfile'\n",
    "generate_key_file(curr_gold_cluster_map, men_type, working_folder, gold_key_file)\n",
    "\n",
    "# group mentions by topic\n",
    "topic_mention_dict = defaultdict(list)\n",
    "for men_id, coref_map in curr_mention_map.items():\n",
    "    topic = coref_map['topic']\n",
    "    topic_mention_dict[topic].append(men_id)\n",
    "\n",
    "# generate mention-pairs\n",
    "mention_pairs = []\n",
    "for mentions in topic_mention_dict.values():\n",
    "    list_mentions = list(mentions)\n",
    "    for i in range(len(list_mentions)):\n",
    "        for j in range(i+1):\n",
    "            if i != j:\n",
    "                mention_pairs.append((list_mentions[i], list_mentions[j]))\n",
    "\n",
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6cb023b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_mention_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "39b28365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "#similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "similarities, _ = get_mention_pair_cosinesimilarity(mention_pairs, all_mention_map,corefbert_vector_map_evt_longbertdoc, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "34aed6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0, 0.48732683)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = np.asarray(similarities)\n",
    "np.max(similarities), np.min(similarities), np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2914672b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d1e5d",
   "metadata": {},
   "source": [
    "# Coref results with cosine sim threshold of 0.6 for coref bert 512 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1d425a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCUB SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (637.927625503153 / 847) 75.31%\tPrecision: (174.610442332893 / 847) 20.61%\tF1: 32.37%\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "MUC SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (480 / 576) 83.33%\tPrecision: (480 / 745) 64.42%\tF1: 72.67%\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "# get indices\n",
    "mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "row, col = zip(*mention_ind_pairs)\n",
    "# print(row, col)\n",
    "\n",
    "# create similarity matrix from the similarities\n",
    "n = len(curr_mentions)\n",
    "similarities = [int(x) for x in similarities ] # to avoid ValueError: row, column, and data arrays must be 1-D\n",
    "similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "# clustering algorithm and mention cluster map\n",
    "clusters, labels = connected_components(similarity_matrix)\n",
    "system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "# generate system key file\n",
    "system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "# evaluate\n",
    "generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1fee0",
   "metadata": {},
   "source": [
    "#  # Coref results with cosine sim threshold of 0.47(mean of original cosine sims) for coref bert 512 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "443efeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCUB SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (844 / 847) 99.64%\tPrecision: (59.5594266564067 / 847) 7.03%\tF1: 13.13%\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "MUC SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (573 / 576) 99.47%\tPrecision: (573 / 843) 67.97%\tF1: 80.76%\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "# get indices\n",
    "mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "row, col = zip(*mention_ind_pairs)\n",
    "# print(row, col)\n",
    "\n",
    "# create similarity matrix from the similarities\n",
    "n = len(curr_mentions)\n",
    "similarities = [int(x) for x in similarities ] # to avoid ValueError: row, column, and data arrays must be 1-D\n",
    "similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "# clustering algorithm and mention cluster map\n",
    "clusters, labels = connected_components(similarity_matrix)\n",
    "system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "# generate system key file\n",
    "system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "# evaluate\n",
    "generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d356e",
   "metadata": {},
   "source": [
    "# coref results for CDLM model with cos sim threshold 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "903de9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eve_mention_map_longdoc_file = working_folder + '/evt_vectorn_map_longbertdoc.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3e1c2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_map_evt_longbertdoc = pickle.load(open(eve_mention_map_longdoc_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "98578d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "similarities, _ = get_mention_pair_cosinesimilarity(mention_pairs, all_mention_map,vector_map_evt_longbertdoc, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "272bac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0, 0.98890084)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = np.asarray(similarities)\n",
    "np.max(similarities), np.min(similarities), np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4f780968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCUB SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (844 / 847) 99.64%\tPrecision: (58.5333491877758 / 847) 6.91%\tF1: 12.92%\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "MUC SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (573 / 576) 99.47%\tPrecision: (573 / 844) 67.89%\tF1: 80.7%\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "# get indices\n",
    "mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "row, col = zip(*mention_ind_pairs)\n",
    "# print(row, col)\n",
    "\n",
    "# create similarity matrix from the similarities\n",
    "n = len(curr_mentions)\n",
    "similarities = [int(x) for x in similarities ] # to avoid ValueError: row, column, and data arrays must be 1-D\n",
    "similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "# clustering algorithm and mention cluster map\n",
    "clusters, labels = connected_components(similarity_matrix)\n",
    "system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "# generate system key file\n",
    "system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "# evaluate\n",
    "generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ca946",
   "metadata": {},
   "source": [
    "# Coref results with CDLM model with lemma similarity, high F1 low recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6538e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "#similarities, _ = get_mention_pair_cosinesimilarity(mention_pairs, all_mention_map,vector_map_evt_longbertdoc, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "468f3e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCUB SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (330.836894933412 / 847) 39.05%\tPrecision: (586.129012327542 / 847) 69.2%\tF1: 49.93%\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "MUC SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (226 / 576) 39.23%\tPrecision: (226 / 399) 56.64%\tF1: 46.35%\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "# get indices\n",
    "mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "row, col = zip(*mention_ind_pairs)\n",
    "# print(row, col)\n",
    "\n",
    "# create similarity matrix from the similarities\n",
    "n = len(curr_mentions)\n",
    "similarities = [int(x) for x in similarities ] # to avoid ValueError: row, column, and data arrays must be 1-D\n",
    "similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "# clustering algorithm and mention cluster map\n",
    "clusters, labels = connected_components(similarity_matrix)\n",
    "system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "# generate system key file\n",
    "system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "# evaluate\n",
    "generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09337925",
   "metadata": {},
   "source": [
    "# BERT large embeddings and coref results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ac81548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/vocab.txt\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/added_tokens.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/special_tokens_map.json\n",
      "loading file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/tokenizer_config.json\n",
      "Adding <m> to the vocabulary\n",
      "Adding </m> to the vocabulary\n",
      "loading configuration file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30524\n",
      "}\n",
      "\n",
      "loading weights file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "cross_encoder = FullCrossEncoderSingle(model_bert_large.config , long=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5d1e8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30524\n",
      "}\n",
      "\n",
      "loading weights file /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at /s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "cross_encoder = cross_encoder.to(device)\n",
    "cross_encoder.model = BertModel.from_pretrained(bert_large_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7ed2f366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullCrossEncoderSingle(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_bert_large_model = torch.nn.DataParallel(cross_encoder)\n",
    "parallel_bert_large_model.module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8c80c7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='/s/chopin/d/proj/ramfis-aida/coreference_and_annotations/coreference/bert_large', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<m>', '</m>']})"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_bert_large_model.eval()\n",
    "parallel_bert_large_model.module.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6493e8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 30523)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_bert_large_model.module.vals[0],parallel_bert_large_model.module.vals[1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "446aa346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([847, 2])\n",
      "torch.Size([847, 10946])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([116, 118]),\n",
       " array([57, 59]),\n",
       " array([142, 144]),\n",
       " array([454, 456]),\n",
       " array([616, 619]),\n",
       " array([334, 338]),\n",
       " array([695, 698]),\n",
       " array([696, 698]),\n",
       " array([858, 861]),\n",
       " array([895, 897]),\n",
       " array([928, 930]),\n",
       " array([928, 930]),\n",
       " array([952, 954]),\n",
       " array([949, 951]),\n",
       " array([975, 977]),\n",
       " array([975, 977]),\n",
       " array([1011, 1013]),\n",
       " array([1038, 1040]),\n",
       " array([1038, 1040]),\n",
       " array([1051, 1053]),\n",
       " array([1073, 1075]),\n",
       " array([1086, 1089]),\n",
       " array([1106, 1108]),\n",
       " array([1106, 1111]),\n",
       " array([222, 227]),\n",
       " array([616, 619]),\n",
       " array([518, 521]),\n",
       " array([962, 964]),\n",
       " array([978, 981]),\n",
       " array([1087, 1090]),\n",
       " array([244, 246]),\n",
       " array([29, 32]),\n",
       " array([380, 382]),\n",
       " array([223, 225]),\n",
       " array([526, 529]),\n",
       " array([542, 545]),\n",
       " array([559, 562]),\n",
       " array([379, 381]),\n",
       " array([533, 536]),\n",
       " array([547, 549]),\n",
       " array([560, 562]),\n",
       " array([436, 439]),\n",
       " array([30, 32]),\n",
       " array([26, 29]),\n",
       " array([26, 29]),\n",
       " array([57, 59]),\n",
       " array([57, 59]),\n",
       " array([117, 120]),\n",
       " array([117, 120]),\n",
       " array([3, 5]),\n",
       " array([43, 45]),\n",
       " array([43, 45]),\n",
       " array([70, 72]),\n",
       " array([70, 72]),\n",
       " array([108, 110]),\n",
       " array([108, 110]),\n",
       " array([122, 125]),\n",
       " array([138, 140]),\n",
       " array([146, 148]),\n",
       " array([164, 166]),\n",
       " array([191, 193]),\n",
       " array([285, 287]),\n",
       " array([318, 320]),\n",
       " array([363, 367]),\n",
       " array([395, 397]),\n",
       " array([381, 384]),\n",
       " array([446, 448]),\n",
       " array([452, 454]),\n",
       " array([456, 458]),\n",
       " array([462, 464]),\n",
       " array([476, 478]),\n",
       " array([478, 480]),\n",
       " array([566, 568]),\n",
       " array([575, 577]),\n",
       " array([577, 580]),\n",
       " array([74, 76]),\n",
       " array([53, 55]),\n",
       " array([74, 76]),\n",
       " array([225, 227]),\n",
       " array([225, 227]),\n",
       " array([179, 181]),\n",
       " array([263, 265]),\n",
       " array([263, 265]),\n",
       " array([19, 21]),\n",
       " array([38, 40]),\n",
       " array([ 9, 11]),\n",
       " array([37, 39]),\n",
       " array([36, 38]),\n",
       " array([63, 65]),\n",
       " array([ 8, 10]),\n",
       " array([7, 9]),\n",
       " array([23, 25]),\n",
       " array([266, 268]),\n",
       " array([45, 47]),\n",
       " array([361, 363]),\n",
       " array([386, 388]),\n",
       " array([404, 406]),\n",
       " array([433, 435]),\n",
       " array([468, 470]),\n",
       " array([466, 468]),\n",
       " array([496, 498]),\n",
       " array([530, 532]),\n",
       " array([615, 617]),\n",
       " array([386, 388]),\n",
       " array([404, 406]),\n",
       " array([411, 413]),\n",
       " array([496, 498]),\n",
       " array([527, 529]),\n",
       " array([615, 617]),\n",
       " array([92, 96]),\n",
       " array([92, 96]),\n",
       " array([144, 146]),\n",
       " array([168, 170]),\n",
       " array([281, 283]),\n",
       " array([286, 288]),\n",
       " array([358, 361]),\n",
       " array([417, 423]),\n",
       " array([34, 36]),\n",
       " array([81, 83]),\n",
       " array([243, 245]),\n",
       " array([302, 304]),\n",
       " array([339, 341]),\n",
       " array([345, 347]),\n",
       " array([355, 357]),\n",
       " array([373, 375]),\n",
       " array([376, 378]),\n",
       " array([396, 398]),\n",
       " array([446, 448]),\n",
       " array([454, 456]),\n",
       " array([471, 473]),\n",
       " array([473, 475]),\n",
       " array([501, 503]),\n",
       " array([507, 509]),\n",
       " array([751, 753]),\n",
       " array([752, 754]),\n",
       " array([958, 960]),\n",
       " array([1123, 1125]),\n",
       " array([1204, 1206]),\n",
       " array([1255, 1258]),\n",
       " array([1275, 1277]),\n",
       " array([1294, 1296]),\n",
       " array([1326, 1328]),\n",
       " array([1342, 1344]),\n",
       " array([1517, 1519]),\n",
       " array([1664, 1667]),\n",
       " array([1697, 1699]),\n",
       " array([1563, 1565]),\n",
       " array([94, 96]),\n",
       " array([728, 730]),\n",
       " array([797, 799]),\n",
       " array([819, 821]),\n",
       " array([871, 873]),\n",
       " array([1155, 1157]),\n",
       " array([1182, 1184]),\n",
       " array([1228, 1230]),\n",
       " array([1277, 1279]),\n",
       " array([1324, 1328]),\n",
       " array([1454, 1456]),\n",
       " array([1489, 1491]),\n",
       " array([1492, 1494]),\n",
       " array([1492, 1494]),\n",
       " array([1523, 1525]),\n",
       " array([1523, 1525]),\n",
       " array([379, 381]),\n",
       " array([980, 982]),\n",
       " array([339, 341]),\n",
       " array([339, 341]),\n",
       " array([325, 329]),\n",
       " array([325, 329]),\n",
       " array([275, 277]),\n",
       " array([276, 278]),\n",
       " array([295, 297]),\n",
       " array([295, 297]),\n",
       " array([302, 304]),\n",
       " array([295, 297]),\n",
       " array([295, 297]),\n",
       " array([302, 304]),\n",
       " array([414, 416]),\n",
       " array([366, 368]),\n",
       " array([381, 384]),\n",
       " array([166, 168]),\n",
       " array([166, 168]),\n",
       " array([57, 59]),\n",
       " array([57, 59]),\n",
       " array([107, 109]),\n",
       " array([107, 109]),\n",
       " array([140, 142]),\n",
       " array([140, 142]),\n",
       " array([179, 181]),\n",
       " array([1555, 1557]),\n",
       " array([1555, 1557]),\n",
       " array([7, 9]),\n",
       " array([7, 9]),\n",
       " array([142, 144]),\n",
       " array([197, 199]),\n",
       " array([197, 199]),\n",
       " array([212, 214]),\n",
       " array([314, 316]),\n",
       " array([ 8, 10]),\n",
       " array([ 8, 10]),\n",
       " array([184, 186]),\n",
       " array([92, 94]),\n",
       " array([ 98, 101]),\n",
       " array([179, 185]),\n",
       " array([303, 307]),\n",
       " array([303, 307]),\n",
       " array([303, 307]),\n",
       " array([303, 307]),\n",
       " array([303, 307]),\n",
       " array([324, 326]),\n",
       " array([193, 196]),\n",
       " array([195, 197]),\n",
       " array([211, 213]),\n",
       " array([211, 213]),\n",
       " array([439, 441]),\n",
       " array([484, 486]),\n",
       " array([571, 573]),\n",
       " array([804, 806]),\n",
       " array([818, 820]),\n",
       " array([818, 820]),\n",
       " array([818, 820]),\n",
       " array([818, 820]),\n",
       " array([171, 173]),\n",
       " array([218, 220]),\n",
       " array([145, 147]),\n",
       " array([710, 712]),\n",
       " array([1264, 1267]),\n",
       " array([415, 418]),\n",
       " array([1109, 1111]),\n",
       " array([73, 75]),\n",
       " array([420, 422]),\n",
       " array([1023, 1025]),\n",
       " array([403, 406]),\n",
       " array([309, 311]),\n",
       " array([655, 657]),\n",
       " array([743, 745]),\n",
       " array([791, 793]),\n",
       " array([249, 252]),\n",
       " array([149, 151]),\n",
       " array([149, 151]),\n",
       " array([289, 291]),\n",
       " array([289, 291]),\n",
       " array([289, 291]),\n",
       " array([289, 291]),\n",
       " array([289, 291]),\n",
       " array([353, 355]),\n",
       " array([353, 355]),\n",
       " array([629, 631]),\n",
       " array([557, 559]),\n",
       " array([74, 76]),\n",
       " array([123, 125]),\n",
       " array([123, 125]),\n",
       " array([225, 228]),\n",
       " array([225, 228]),\n",
       " array([340, 342]),\n",
       " array([476, 478]),\n",
       " array([489, 492]),\n",
       " array([516, 518]),\n",
       " array([586, 588]),\n",
       " array([599, 601]),\n",
       " array([863, 866]),\n",
       " array([1902, 1904]),\n",
       " array([1982, 1984]),\n",
       " array([736, 738]),\n",
       " array([38, 41]),\n",
       " array([393, 396]),\n",
       " array([434, 436]),\n",
       " array([531, 534]),\n",
       " array([537, 539]),\n",
       " array([140, 144]),\n",
       " array([800, 802]),\n",
       " array([108, 111]),\n",
       " array([640, 643]),\n",
       " array([646, 648]),\n",
       " array([1, 3]),\n",
       " array([202, 204]),\n",
       " array([234, 236]),\n",
       " array([351, 353]),\n",
       " array([218, 220]),\n",
       " array([405, 407]),\n",
       " array([494, 496]),\n",
       " array([803, 805]),\n",
       " array([997, 999]),\n",
       " array([54, 56]),\n",
       " array([45, 47]),\n",
       " array([133, 135]),\n",
       " array([139, 141]),\n",
       " array([139, 141]),\n",
       " array([17, 19]),\n",
       " array([359, 361]),\n",
       " array([447, 450]),\n",
       " array([951, 953]),\n",
       " array([447, 450]),\n",
       " array([139, 141]),\n",
       " array([139, 141]),\n",
       " array([587, 590]),\n",
       " array([1091, 1093]),\n",
       " array([20, 22]),\n",
       " array([427, 429]),\n",
       " array([443, 445]),\n",
       " array([466, 469]),\n",
       " array([559, 561]),\n",
       " array([559, 561]),\n",
       " array([823, 825]),\n",
       " array([34, 36]),\n",
       " array([635, 638]),\n",
       " array([299, 301]),\n",
       " array([824, 826]),\n",
       " array([800, 802]),\n",
       " array([90, 93]),\n",
       " array([663, 665]),\n",
       " array([765, 767]),\n",
       " array([635, 638]),\n",
       " array([431, 434]),\n",
       " array([938, 940]),\n",
       " array([958, 960]),\n",
       " array([194, 196]),\n",
       " array([235, 237]),\n",
       " array([447, 449]),\n",
       " array([82, 84]),\n",
       " array([554, 556]),\n",
       " array([71, 74]),\n",
       " array([984, 986]),\n",
       " array([1232, 1234]),\n",
       " array([1410, 1413]),\n",
       " array([2106, 2108]),\n",
       " array([2102, 2104]),\n",
       " array([2195, 2197]),\n",
       " array([2222, 2224]),\n",
       " array([79, 81]),\n",
       " array([1651, 1654]),\n",
       " array([1651, 1654]),\n",
       " array([123, 125]),\n",
       " array([92, 94]),\n",
       " array([394, 397]),\n",
       " array([29, 31]),\n",
       " array([48, 50]),\n",
       " array([133, 135]),\n",
       " array([133, 135]),\n",
       " array([199, 201]),\n",
       " array([208, 210]),\n",
       " array([810, 812]),\n",
       " array([666, 668]),\n",
       " array([ 9, 11]),\n",
       " array([91, 93]),\n",
       " array([ 9, 11]),\n",
       " array([61, 63]),\n",
       " array([61, 63]),\n",
       " array([407, 409]),\n",
       " array([61, 63]),\n",
       " array([61, 63]),\n",
       " array([967, 969]),\n",
       " array([0, 3]),\n",
       " array([4, 6]),\n",
       " array([11, 14]),\n",
       " array([15, 17]),\n",
       " array([41, 43]),\n",
       " array([38, 41]),\n",
       " array([102, 104]),\n",
       " array([179, 181]),\n",
       " array([362, 364]),\n",
       " array([519, 521]),\n",
       " array([618, 620]),\n",
       " array([639, 641]),\n",
       " array([652, 654]),\n",
       " array([665, 667]),\n",
       " array([710, 712]),\n",
       " array([1127, 1129]),\n",
       " array([1293, 1295]),\n",
       " array([1346, 1348]),\n",
       " array([254, 256]),\n",
       " array([10859, 10863]),\n",
       " array([10843, 10846]),\n",
       " array([10781, 10784]),\n",
       " array([10252, 10254]),\n",
       " array([8975, 8977]),\n",
       " array([10504, 10507]),\n",
       " array([10453, 10455]),\n",
       " array([9723, 9726]),\n",
       " array([8908, 8910]),\n",
       " array([7550, 7552]),\n",
       " array([6729, 6733]),\n",
       " array([6764, 6767]),\n",
       " array([6772, 6774]),\n",
       " array([3293, 3296]),\n",
       " array([2415, 2417]),\n",
       " array([2261, 2263]),\n",
       " array([1650, 1653]),\n",
       " array([1122, 1124]),\n",
       " array([5670, 5674]),\n",
       " array([1650, 1653]),\n",
       " array([1122, 1124]),\n",
       " array([1122, 1124]),\n",
       " array([1122, 1124]),\n",
       " array([9841, 9843]),\n",
       " array([9372, 9374]),\n",
       " array([9229, 9231]),\n",
       " array([9318, 9320]),\n",
       " array([8628, 8630]),\n",
       " array([8512, 8514]),\n",
       " array([8652, 8654]),\n",
       " array([8023, 8025]),\n",
       " array([6903, 6905]),\n",
       " array([5350, 5352]),\n",
       " array([4165, 4167]),\n",
       " array([181, 183]),\n",
       " array([443, 445]),\n",
       " array([190, 192]),\n",
       " array([404, 407]),\n",
       " array([240, 242]),\n",
       " array([276, 282]),\n",
       " array([855, 858]),\n",
       " array([430, 432]),\n",
       " array([537, 539]),\n",
       " array([254, 256]),\n",
       " array([624, 627]),\n",
       " array([697, 701]),\n",
       " array([749, 752]),\n",
       " array([72, 74]),\n",
       " array([97, 99]),\n",
       " array([515, 517]),\n",
       " array([340, 343]),\n",
       " array([581, 583]),\n",
       " array([533, 535]),\n",
       " array([533, 535]),\n",
       " array([645, 647]),\n",
       " array([694, 696]),\n",
       " array([57, 59]),\n",
       " array([121, 124]),\n",
       " array([180, 182]),\n",
       " array([384, 386]),\n",
       " array([329, 331]),\n",
       " array([651, 653]),\n",
       " array([36, 38]),\n",
       " array([871, 873]),\n",
       " array([965, 967]),\n",
       " array([66, 69]),\n",
       " array([100, 102]),\n",
       " array([403, 405]),\n",
       " array([727, 731]),\n",
       " array([442, 445]),\n",
       " array([319, 321]),\n",
       " array([260, 263]),\n",
       " array([77, 79]),\n",
       " array([177, 179]),\n",
       " array([771, 773]),\n",
       " array([6, 8]),\n",
       " array([1083, 1085]),\n",
       " array([1226, 1228]),\n",
       " array([202, 205]),\n",
       " array([70, 72]),\n",
       " array([117, 119]),\n",
       " array([504, 506]),\n",
       " array([508, 510]),\n",
       " array([584, 586]),\n",
       " array([647, 649]),\n",
       " array([565, 567]),\n",
       " array([691, 693]),\n",
       " array([699, 702]),\n",
       " array([728, 730]),\n",
       " array([731, 733]),\n",
       " array([787, 789]),\n",
       " array([796, 798]),\n",
       " array([899, 901]),\n",
       " array([940, 945]),\n",
       " array([992, 994]),\n",
       " array([1022, 1024]),\n",
       " array([15, 17]),\n",
       " array([1140, 1145]),\n",
       " array([1047, 1049]),\n",
       " array([1232, 1234]),\n",
       " array([33, 38]),\n",
       " array([1428, 1430]),\n",
       " array([552, 554]),\n",
       " array([797, 799]),\n",
       " array([907, 909]),\n",
       " array([940, 942]),\n",
       " array([1205, 1207]),\n",
       " array([1116, 1118]),\n",
       " array([178, 180]),\n",
       " array([208, 210]),\n",
       " array([242, 244]),\n",
       " array([259, 261]),\n",
       " array([281, 283]),\n",
       " array([283, 285]),\n",
       " array([329, 331]),\n",
       " array([889, 893]),\n",
       " array([973, 976]),\n",
       " array([1004, 1006]),\n",
       " array([1017, 1021]),\n",
       " array([1389, 1391]),\n",
       " array([1401, 1403]),\n",
       " array([180, 182]),\n",
       " array([191, 193]),\n",
       " array([242, 244]),\n",
       " array([355, 357]),\n",
       " array([378, 384]),\n",
       " array([530, 532]),\n",
       " array([786, 788]),\n",
       " array([855, 857]),\n",
       " array([943, 945]),\n",
       " array([1004, 1007]),\n",
       " array([1010, 1012]),\n",
       " array([1194, 1196]),\n",
       " array([1266, 1268]),\n",
       " array([1831, 1833]),\n",
       " array([1888, 1890]),\n",
       " array([1943, 1945]),\n",
       " array([430, 432]),\n",
       " array([1907, 1913]),\n",
       " array([399, 415]),\n",
       " array([536, 547]),\n",
       " array([754, 766]),\n",
       " array([956, 964]),\n",
       " array([1011, 1015]),\n",
       " array([1026, 1034]),\n",
       " array([1114, 1121]),\n",
       " array([1122, 1134]),\n",
       " array([1269, 1272]),\n",
       " array([1585, 1591]),\n",
       " array([1821, 1837]),\n",
       " array([1886, 1894]),\n",
       " array([1904, 1918]),\n",
       " array([1935, 1953]),\n",
       " array([1982, 1996]),\n",
       " array([2000, 2013]),\n",
       " array([2030, 2045]),\n",
       " array([2134, 2142]),\n",
       " array([2248, 2263]),\n",
       " array([2299, 2315]),\n",
       " array([2408, 2432]),\n",
       " array([2467, 2488]),\n",
       " array([2498, 2530]),\n",
       " array([2498, 2530]),\n",
       " array([2530, 2540]),\n",
       " array([2540, 2565]),\n",
       " array([2566, 2584]),\n",
       " array([2630, 2643]),\n",
       " array([2734, 2753]),\n",
       " array([2769, 2816]),\n",
       " array([2895, 2905]),\n",
       " array([2909, 2922]),\n",
       " array([2963, 2981]),\n",
       " array([3026, 3042]),\n",
       " array([3138, 3147]),\n",
       " array([3415, 3433]),\n",
       " array([3448, 3500]),\n",
       " array([3543, 3572]),\n",
       " array([3698, 3708]),\n",
       " array([3796, 3814]),\n",
       " array([3837, 3889]),\n",
       " array([4356, 4374]),\n",
       " array([4461, 4483]),\n",
       " array([4506, 4528]),\n",
       " array([4539, 4558]),\n",
       " array([5150, 5159]),\n",
       " array([5203, 5218]),\n",
       " array([5301, 5326]),\n",
       " array([5405, 5407]),\n",
       " array([5414, 5435]),\n",
       " array([5991, 6004]),\n",
       " array([6025, 6027]),\n",
       " array([6046, 6048]),\n",
       " array([455, 457]),\n",
       " array([1060, 1062]),\n",
       " array([1084, 1086]),\n",
       " array([1071, 1073]),\n",
       " array([51, 55]),\n",
       " array([462, 464]),\n",
       " array([914, 916]),\n",
       " array([914, 916]),\n",
       " array([455, 457]),\n",
       " array([924, 926]),\n",
       " array([1036, 1038]),\n",
       " array([1053, 1055]),\n",
       " array([1223, 1225]),\n",
       " array([336, 338]),\n",
       " array([1084, 1086]),\n",
       " array([35, 41]),\n",
       " array([22, 24]),\n",
       " array([79, 81]),\n",
       " array([173, 175]),\n",
       " array([353, 360]),\n",
       " array([374, 376]),\n",
       " array([390, 392]),\n",
       " array([707, 709]),\n",
       " array([712, 714]),\n",
       " array([730, 732]),\n",
       " array([67, 69]),\n",
       " array([67, 69]),\n",
       " array([86, 88]),\n",
       " array([161, 163]),\n",
       " array([163, 165]),\n",
       " array([196, 198]),\n",
       " array([86, 88]),\n",
       " array([114, 116]),\n",
       " array([18, 20]),\n",
       " array([18, 20]),\n",
       " array([14, 16]),\n",
       " array([161, 163]),\n",
       " array([163, 165]),\n",
       " array([4, 6]),\n",
       " array([113, 115]),\n",
       " array([116, 118]),\n",
       " array([135, 138]),\n",
       " array([194, 198]),\n",
       " array([225, 230]),\n",
       " array([232, 235]),\n",
       " array([383, 387]),\n",
       " array([540, 542]),\n",
       " array([596, 598]),\n",
       " array([630, 632]),\n",
       " array([732, 734]),\n",
       " array([840, 843]),\n",
       " array([881, 883]),\n",
       " array([1005, 1007]),\n",
       " array([1175, 1178]),\n",
       " array([1490, 1495]),\n",
       " array([1522, 1525]),\n",
       " array([1621, 1624]),\n",
       " array([1644, 1647]),\n",
       " array([1650, 1652]),\n",
       " array([1682, 1688]),\n",
       " array([1950, 1954]),\n",
       " array([210, 212]),\n",
       " array([223, 225]),\n",
       " array([228, 230]),\n",
       " array([225, 227]),\n",
       " array([6, 8]),\n",
       " array([332, 334]),\n",
       " array([332, 334]),\n",
       " array([44, 46]),\n",
       " array([181, 183]),\n",
       " array([181, 183]),\n",
       " array([115, 117]),\n",
       " array([131, 134]),\n",
       " array([131, 134]),\n",
       " array([191, 193]),\n",
       " array([168, 170]),\n",
       " array([198, 202]),\n",
       " array([220, 222]),\n",
       " array([225, 227]),\n",
       " array([401, 403]),\n",
       " array([401, 403]),\n",
       " array([209, 211]),\n",
       " array([207, 209]),\n",
       " array([212, 217]),\n",
       " array([207, 209]),\n",
       " array([691, 693]),\n",
       " array([699, 701]),\n",
       " array([165, 168]),\n",
       " array([37, 39]),\n",
       " array([14, 16]),\n",
       " array([222, 224]),\n",
       " array([270, 272]),\n",
       " array([454, 456]),\n",
       " array([457, 459]),\n",
       " array([463, 465]),\n",
       " array([361, 363]),\n",
       " array([364, 366]),\n",
       " array([587, 589]),\n",
       " array([642, 644]),\n",
       " array([1061, 1063]),\n",
       " array([1086, 1088]),\n",
       " array([1014, 1017]),\n",
       " array([175, 177]),\n",
       " array([182, 184]),\n",
       " array([298, 300]),\n",
       " array([40, 42]),\n",
       " array([32, 34]),\n",
       " array([228, 230]),\n",
       " array([276, 278]),\n",
       " array([382, 384]),\n",
       " array([420, 423]),\n",
       " array([434, 436]),\n",
       " array([438, 442]),\n",
       " array([620, 622]),\n",
       " array([507, 509]),\n",
       " array([507, 509]),\n",
       " array([523, 525]),\n",
       " array([537, 539]),\n",
       " array([541, 543]),\n",
       " array([561, 564]),\n",
       " array([587, 589]),\n",
       " array([631, 633]),\n",
       " array([633, 635]),\n",
       " array([11, 13]),\n",
       " array([11, 13]),\n",
       " array([40, 42]),\n",
       " array([1401, 1404]),\n",
       " array([1420, 1422]),\n",
       " array([1426, 1428]),\n",
       " array([1438, 1440]),\n",
       " array([1453, 1455]),\n",
       " array([1536, 1538]),\n",
       " array([1538, 1541]),\n",
       " array([1597, 1600]),\n",
       " array([1603, 1606]),\n",
       " array([1627, 1630]),\n",
       " array([1647, 1652]),\n",
       " array([1652, 1654]),\n",
       " array([1671, 1673]),\n",
       " array([1742, 1749]),\n",
       " array([1789, 1791]),\n",
       " array([2326, 2328]),\n",
       " array([2679, 2681]),\n",
       " array([2736, 2738]),\n",
       " array([2835, 2837]),\n",
       " array([2899, 2901]),\n",
       " array([2901, 2903]),\n",
       " array([2908, 2910]),\n",
       " array([3082, 3087]),\n",
       " array([3945, 3947]),\n",
       " array([4171, 4175]),\n",
       " array([5620, 5622]),\n",
       " array([5963, 5965]),\n",
       " array([5985, 5987]),\n",
       " array([6018, 6020]),\n",
       " array([6463, 6466]),\n",
       " array([6891, 6895]),\n",
       " array([6894, 6896]),\n",
       " array([7015, 7017]),\n",
       " array([7015, 7017]),\n",
       " array([7063, 7065]),\n",
       " array([7069, 7071]),\n",
       " array([7069, 7071]),\n",
       " array([36, 38]),\n",
       " array([154, 156]),\n",
       " array([147, 149]),\n",
       " array([189, 192]),\n",
       " array([189, 192]),\n",
       " array([209, 211]),\n",
       " array([217, 219]),\n",
       " array([247, 249]),\n",
       " array([247, 249]),\n",
       " array([375, 377]),\n",
       " array([384, 386]),\n",
       " array([390, 392]),\n",
       " array([624, 626]),\n",
       " array([625, 627]),\n",
       " array([384, 386]),\n",
       " array([428, 431]),\n",
       " array([443, 445]),\n",
       " array([458, 460]),\n",
       " array([467, 469]),\n",
       " array([717, 719]),\n",
       " array([761, 763]),\n",
       " array([797, 799]),\n",
       " array([128, 131]),\n",
       " array([131, 133]),\n",
       " array([87, 90]),\n",
       " array([148, 150]),\n",
       " array([195, 198]),\n",
       " array([265, 267]),\n",
       " array([246, 248]),\n",
       " array([322, 324]),\n",
       " array([311, 313]),\n",
       " array([386, 388]),\n",
       " array([408, 410]),\n",
       " array([441, 443]),\n",
       " array([457, 459]),\n",
       " array([477, 479]),\n",
       " array([494, 496]),\n",
       " array([527, 529]),\n",
       " array([527, 529]),\n",
       " array([577, 579]),\n",
       " array([577, 579]),\n",
       " array([664, 666]),\n",
       " array([604, 606]),\n",
       " array([32, 35]),\n",
       " array([36, 38]),\n",
       " array([64, 66]),\n",
       " array([320, 322]),\n",
       " array([408, 410]),\n",
       " array([94, 96]),\n",
       " array([137, 139]),\n",
       " array([303, 305]),\n",
       " array([462, 464]),\n",
       " array([84, 86]),\n",
       " array([50, 52]),\n",
       " array([ 9, 11]),\n",
       " array([271, 273]),\n",
       " array([545, 547]),\n",
       " array([64, 66]),\n",
       " array([77, 79]),\n",
       " array([225, 227]),\n",
       " array([225, 227]),\n",
       " array([108, 110]),\n",
       " array([156, 158]),\n",
       " array([192, 194]),\n",
       " array([11, 13]),\n",
       " array([232, 234]),\n",
       " array([382, 384]),\n",
       " array([451, 453]),\n",
       " array([431, 433]),\n",
       " array([217, 219]),\n",
       " array([217, 219]),\n",
       " array([220, 222]),\n",
       " array([220, 222]),\n",
       " array([258, 260]),\n",
       " array([258, 260]),\n",
       " array([524, 526]),\n",
       " array([495, 497]),\n",
       " array([495, 497]),\n",
       " array([524, 526]),\n",
       " array([408, 410]),\n",
       " array([406, 408]),\n",
       " array([329, 331]),\n",
       " array([350, 355]),\n",
       " array([436, 438]),\n",
       " array([133, 135]),\n",
       " array([133, 135]),\n",
       " array([43, 45]),\n",
       " array([53, 55]),\n",
       " array([365, 368]),\n",
       " array([378, 380]),\n",
       " array([384, 386]),\n",
       " array([388, 390]),\n",
       " array([328, 330]),\n",
       " array([337, 340]),\n",
       " array([43, 46]),\n",
       " array([79, 82]),\n",
       " array([106, 108]),\n",
       " array([132, 136]),\n",
       " array([132, 136]),\n",
       " array([153, 156]),\n",
       " array([164, 172]),\n",
       " array([189, 192]),\n",
       " array([192, 196]),\n",
       " array([723, 727]),\n",
       " array([15, 19]),\n",
       " array([194, 196]),\n",
       " array([181, 184]),\n",
       " array([150, 152]),\n",
       " array([169, 171]),\n",
       " array([211, 213]),\n",
       " array([226, 228]),\n",
       " array([238, 240]),\n",
       " array([253, 255]),\n",
       " array([245, 247]),\n",
       " array([1, 3]),\n",
       " array([157, 159]),\n",
       " array([1, 3]),\n",
       " array([157, 159]),\n",
       " array([483, 485]),\n",
       " array([489, 491]),\n",
       " array([479, 481])]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_mentions = []\n",
    "bert_sentences\n",
    "batch = [bert_sentences[x] for x in range( len(bert_sentences))]\n",
    " \n",
    "bert_tokens = parallel_bert_large_model.module.tokenizer(batch, pad_to_max_length=True, add_special_tokens=False, truncation =False )\n",
    "#bert_tokens = parallel_model.module.tokenizer(batch, padding = 'max_length', add_special_tokens=False, truncation =True )\n",
    "input_ids = torch.tensor(bert_tokens['input_ids'], device=device)\n",
    "attention_mask = torch.tensor(bert_tokens['attention_mask'], device=device)\n",
    "m = input_ids.cpu()\n",
    "k = m == parallel_bert_large_model.module.vals[0]\n",
    "p = m == parallel_bert_large_model.module.vals[1]\n",
    "v = (k.int() + p.int()).bool()\n",
    "nz_indexes = v.nonzero()[:, 1].reshape(m.shape[0], 2)\n",
    "print(nz_indexes.shape)\n",
    "#print(nz_indexes[0][1])\n",
    "loc_mentions.extend(nz_indexes.cpu().numpy())\n",
    "print(input_ids.shape)\n",
    "loc_mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "39eef983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 619]\n",
      "m shape torch.Size([512]) 4\n",
      "740\n",
      "[6, 698]\n",
      "m shape torch.Size([512]) 6\n",
      "740\n",
      "[7, 698]\n",
      "m shape torch.Size([512]) 7\n",
      "740\n",
      "[8, 861]\n",
      "m shape torch.Size([512]) 8\n",
      "740\n",
      "[9, 897]\n",
      "m shape torch.Size([512]) 9\n",
      "740\n",
      "[10, 930]\n",
      "m shape torch.Size([512]) 10\n",
      "741\n",
      "[11, 930]\n",
      "m shape torch.Size([512]) 11\n",
      "741\n",
      "[12, 954]\n",
      "m shape torch.Size([512]) 12\n",
      "741\n",
      "[13, 951]\n",
      "m shape torch.Size([512]) 13\n",
      "740\n",
      "[14, 977]\n",
      "m shape torch.Size([512]) 14\n",
      "740\n",
      "[15, 977]\n",
      "m shape torch.Size([512]) 15\n",
      "740\n",
      "[16, 1013]\n",
      "m shape torch.Size([512]) 16\n",
      "740\n",
      "[17, 1040]\n",
      "m shape torch.Size([512]) 17\n",
      "740\n",
      "[18, 1040]\n",
      "m shape torch.Size([512]) 18\n",
      "740\n",
      "[19, 1053]\n",
      "m shape torch.Size([512]) 19\n",
      "740\n",
      "[20, 1075]\n",
      "m shape torch.Size([512]) 20\n",
      "740\n",
      "[21, 1089]\n",
      "m shape torch.Size([512]) 21\n",
      "740\n",
      "[22, 1108]\n",
      "m shape torch.Size([512]) 22\n",
      "740\n",
      "[23, 1111]\n",
      "m shape torch.Size([512]) 23\n",
      "740\n",
      "[25, 619]\n",
      "m shape torch.Size([512]) 25\n",
      "740\n",
      "[26, 521]\n",
      "m shape torch.Size([512]) 26\n",
      "740\n",
      "[27, 964]\n",
      "m shape torch.Size([512]) 27\n",
      "741\n",
      "[28, 981]\n",
      "m shape torch.Size([512]) 28\n",
      "741\n",
      "[29, 1090]\n",
      "m shape torch.Size([512]) 29\n",
      "741\n",
      "[34, 529]\n",
      "m shape torch.Size([512]) 34\n",
      "721\n",
      "[35, 545]\n",
      "m shape torch.Size([512]) 35\n",
      "721\n",
      "[36, 562]\n",
      "m shape torch.Size([512]) 36\n",
      "722\n",
      "[38, 536]\n",
      "m shape torch.Size([512]) 38\n",
      "721\n",
      "[39, 549]\n",
      "m shape torch.Size([512]) 39\n",
      "721\n",
      "[40, 562]\n",
      "m shape torch.Size([512]) 40\n",
      "722\n",
      "[72, 568]\n",
      "m shape torch.Size([512]) 72\n",
      "418\n",
      "[73, 577]\n",
      "m shape torch.Size([512]) 73\n",
      "418\n",
      "[74, 580]\n",
      "m shape torch.Size([512]) 74\n",
      "418\n",
      "[101, 532]\n",
      "m shape torch.Size([512]) 101\n",
      "767\n",
      "[102, 617]\n",
      "m shape torch.Size([512]) 102\n",
      "767\n",
      "[107, 529]\n",
      "m shape torch.Size([512]) 107\n",
      "767\n",
      "[108, 617]\n",
      "m shape torch.Size([512]) 108\n",
      "767\n",
      "[133, 753]\n",
      "m shape torch.Size([512]) 133\n",
      "1353\n",
      "[134, 754]\n",
      "m shape torch.Size([512]) 134\n",
      "1353\n",
      "[135, 960]\n",
      "m shape torch.Size([512]) 135\n",
      "1353\n",
      "[136, 1125]\n",
      "m shape torch.Size([512]) 136\n",
      "1353\n",
      "[137, 1206]\n",
      "m shape torch.Size([512]) 137\n",
      "1353\n",
      "[138, 1258]\n",
      "m shape torch.Size([512]) 138\n",
      "1353\n",
      "[139, 1277]\n",
      "m shape torch.Size([512]) 139\n",
      "1353\n",
      "[140, 1296]\n",
      "m shape torch.Size([512]) 140\n",
      "1353\n",
      "[141, 1328]\n",
      "m shape torch.Size([512]) 141\n",
      "1353\n",
      "[142, 1344]\n",
      "m shape torch.Size([512]) 142\n",
      "1353\n",
      "[143, 1519]\n",
      "m shape torch.Size([512]) 143\n",
      "1354\n",
      "[144, 1667]\n",
      "m shape torch.Size([512]) 144\n",
      "1353\n",
      "[145, 1699]\n",
      "m shape torch.Size([512]) 145\n",
      "1354\n",
      "[146, 1565]\n",
      "m shape torch.Size([512]) 146\n",
      "1353\n",
      "[148, 730]\n",
      "m shape torch.Size([512]) 148\n",
      "1264\n",
      "[149, 799]\n",
      "m shape torch.Size([512]) 149\n",
      "1264\n",
      "[150, 821]\n",
      "m shape torch.Size([512]) 150\n",
      "1264\n",
      "[151, 873]\n",
      "m shape torch.Size([512]) 151\n",
      "1264\n",
      "[152, 1157]\n",
      "m shape torch.Size([512]) 152\n",
      "1265\n",
      "[153, 1184]\n",
      "m shape torch.Size([512]) 153\n",
      "1264\n",
      "[154, 1230]\n",
      "m shape torch.Size([512]) 154\n",
      "1264\n",
      "[155, 1279]\n",
      "m shape torch.Size([512]) 155\n",
      "1264\n",
      "[156, 1328]\n",
      "m shape torch.Size([512]) 156\n",
      "1264\n",
      "[157, 1456]\n",
      "m shape torch.Size([512]) 157\n",
      "1265\n",
      "[158, 1491]\n",
      "m shape torch.Size([512]) 158\n",
      "1264\n",
      "[159, 1494]\n",
      "m shape torch.Size([512]) 159\n",
      "1264\n",
      "[160, 1494]\n",
      "m shape torch.Size([512]) 160\n",
      "1264\n",
      "[161, 1525]\n",
      "m shape torch.Size([512]) 161\n",
      "1264\n",
      "[162, 1525]\n",
      "m shape torch.Size([512]) 162\n",
      "1264\n",
      "[164, 982]\n",
      "m shape torch.Size([512]) 164\n",
      "1264\n",
      "[189, 1557]\n",
      "m shape torch.Size([512]) 189\n",
      "1213\n",
      "[190, 1557]\n",
      "m shape torch.Size([512]) 190\n",
      "1213\n",
      "[216, 573]\n",
      "m shape torch.Size([512]) 216\n",
      "743\n",
      "[217, 806]\n",
      "m shape torch.Size([512]) 217\n",
      "743\n",
      "[218, 820]\n",
      "m shape torch.Size([512]) 218\n",
      "743\n",
      "[219, 820]\n",
      "m shape torch.Size([512]) 219\n",
      "743\n",
      "[220, 820]\n",
      "m shape torch.Size([512]) 220\n",
      "743\n",
      "[221, 820]\n",
      "m shape torch.Size([512]) 221\n",
      "743\n",
      "[225, 712]\n",
      "m shape torch.Size([512]) 225\n",
      "743\n",
      "[226, 1267]\n",
      "m shape torch.Size([512]) 226\n",
      "2180\n",
      "[228, 1111]\n",
      "m shape torch.Size([512]) 228\n",
      "2180\n",
      "[231, 1025]\n",
      "m shape torch.Size([512]) 231\n",
      "1011\n",
      "[234, 657]\n",
      "m shape torch.Size([512]) 234\n",
      "759\n",
      "[235, 745]\n",
      "m shape torch.Size([512]) 235\n",
      "759\n",
      "[236, 793]\n",
      "m shape torch.Size([512]) 236\n",
      "759\n",
      "[247, 631]\n",
      "m shape torch.Size([512]) 247\n",
      "759\n",
      "[248, 559]\n",
      "m shape torch.Size([512]) 248\n",
      "759\n",
      "[257, 518]\n",
      "m shape torch.Size([512]) 257\n",
      "1775\n",
      "[258, 588]\n",
      "m shape torch.Size([512]) 258\n",
      "1775\n",
      "[259, 601]\n",
      "m shape torch.Size([512]) 259\n",
      "1776\n",
      "[260, 866]\n",
      "m shape torch.Size([512]) 260\n",
      "1775\n",
      "[261, 1904]\n",
      "m shape torch.Size([512]) 261\n",
      "1775\n",
      "[262, 1984]\n",
      "m shape torch.Size([512]) 262\n",
      "1776\n",
      "[263, 738]\n",
      "m shape torch.Size([512]) 263\n",
      "1775\n",
      "[267, 534]\n",
      "m shape torch.Size([512]) 267\n",
      "1043\n",
      "[268, 539]\n",
      "m shape torch.Size([512]) 268\n",
      "1043\n",
      "[270, 802]\n",
      "m shape torch.Size([512]) 270\n",
      "1043\n",
      "[272, 643]\n",
      "m shape torch.Size([512]) 272\n",
      "1043\n",
      "[273, 648]\n",
      "m shape torch.Size([512]) 273\n",
      "1044\n",
      "[281, 805]\n",
      "m shape torch.Size([512]) 281\n",
      "930\n",
      "[282, 999]\n",
      "m shape torch.Size([512]) 282\n",
      "930\n",
      "[291, 953]\n",
      "m shape torch.Size([512]) 291\n",
      "1120\n",
      "[295, 590]\n",
      "m shape torch.Size([512]) 295\n",
      "1121\n",
      "[296, 1093]\n",
      "m shape torch.Size([512]) 296\n",
      "1120\n",
      "[301, 561]\n",
      "m shape torch.Size([512]) 301\n",
      "713\n",
      "[302, 561]\n",
      "m shape torch.Size([512]) 302\n",
      "713\n",
      "[303, 825]\n",
      "m shape torch.Size([512]) 303\n",
      "713\n",
      "[305, 638]\n",
      "m shape torch.Size([512]) 305\n",
      "713\n",
      "[307, 826]\n",
      "m shape torch.Size([512]) 307\n",
      "714\n",
      "[308, 802]\n",
      "m shape torch.Size([512]) 308\n",
      "713\n",
      "[310, 665]\n",
      "m shape torch.Size([512]) 310\n",
      "713\n",
      "[311, 767]\n",
      "m shape torch.Size([512]) 311\n",
      "713\n",
      "[312, 638]\n",
      "m shape torch.Size([512]) 312\n",
      "713\n",
      "[314, 940]\n",
      "m shape torch.Size([512]) 314\n",
      "778\n",
      "[315, 960]\n",
      "m shape torch.Size([512]) 315\n",
      "779\n",
      "[320, 556]\n",
      "m shape torch.Size([512]) 320\n",
      "778\n",
      "[322, 986]\n",
      "m shape torch.Size([512]) 322\n",
      "1849\n",
      "[323, 1234]\n",
      "m shape torch.Size([512]) 323\n",
      "1848\n",
      "[324, 1413]\n",
      "m shape torch.Size([512]) 324\n",
      "1848\n",
      "[325, 2108]\n",
      "m shape torch.Size([512]) 325\n",
      "1848\n",
      "[326, 2104]\n",
      "m shape torch.Size([512]) 326\n",
      "1848\n",
      "[327, 2197]\n",
      "m shape torch.Size([512]) 327\n",
      "1848\n",
      "[328, 2224]\n",
      "m shape torch.Size([512]) 328\n",
      "1848\n",
      "[330, 1654]\n",
      "m shape torch.Size([512]) 330\n",
      "1848\n",
      "[331, 1654]\n",
      "m shape torch.Size([512]) 331\n",
      "1848\n",
      "[341, 812]\n",
      "m shape torch.Size([512]) 341\n",
      "690\n",
      "[342, 668]\n",
      "m shape torch.Size([512]) 342\n",
      "689\n",
      "[351, 969]\n",
      "m shape torch.Size([512]) 351\n",
      "962\n",
      "[361, 521]\n",
      "m shape torch.Size([512]) 361\n",
      "1080\n",
      "[362, 620]\n",
      "m shape torch.Size([512]) 362\n",
      "1080\n",
      "[363, 641]\n",
      "m shape torch.Size([512]) 363\n",
      "1080\n",
      "[364, 654]\n",
      "m shape torch.Size([512]) 364\n",
      "1081\n",
      "[365, 667]\n",
      "m shape torch.Size([512]) 365\n",
      "1081\n",
      "[366, 712]\n",
      "m shape torch.Size([512]) 366\n",
      "1080\n",
      "[367, 1129]\n",
      "m shape torch.Size([512]) 367\n",
      "1080\n",
      "[368, 1295]\n",
      "m shape torch.Size([512]) 368\n",
      "1080\n",
      "[369, 1348]\n",
      "m shape torch.Size([512]) 369\n",
      "1081\n",
      "[371, 10863]\n",
      "m shape torch.Size([512]) 371\n",
      "7192\n",
      "[372, 10846]\n",
      "m shape torch.Size([512]) 372\n",
      "7192\n",
      "[373, 10784]\n",
      "m shape torch.Size([512]) 373\n",
      "7192\n",
      "[374, 10254]\n",
      "m shape torch.Size([512]) 374\n",
      "7192\n",
      "[375, 8977]\n",
      "m shape torch.Size([512]) 375\n",
      "7192\n",
      "[376, 10507]\n",
      "m shape torch.Size([512]) 376\n",
      "7192\n",
      "[377, 10455]\n",
      "m shape torch.Size([512]) 377\n",
      "7192\n",
      "[378, 9726]\n",
      "m shape torch.Size([512]) 378\n",
      "7192\n",
      "[379, 8910]\n",
      "m shape torch.Size([512]) 379\n",
      "7192\n",
      "[380, 7552]\n",
      "m shape torch.Size([512]) 380\n",
      "7192\n",
      "[381, 6733]\n",
      "m shape torch.Size([512]) 381\n",
      "7192\n",
      "[382, 6767]\n",
      "m shape torch.Size([512]) 382\n",
      "7192\n",
      "[383, 6774]\n",
      "m shape torch.Size([512]) 383\n",
      "7192\n",
      "[384, 3296]\n",
      "m shape torch.Size([512]) 384\n",
      "7192\n",
      "[385, 2417]\n",
      "m shape torch.Size([512]) 385\n",
      "7192\n",
      "[386, 2263]\n",
      "m shape torch.Size([512]) 386\n",
      "7192\n",
      "[387, 1653]\n",
      "m shape torch.Size([512]) 387\n",
      "7192\n",
      "[388, 1124]\n",
      "m shape torch.Size([512]) 388\n",
      "7192\n",
      "[389, 5674]\n",
      "m shape torch.Size([512]) 389\n",
      "7192\n",
      "[390, 1653]\n",
      "m shape torch.Size([512]) 390\n",
      "7192\n",
      "[391, 1124]\n",
      "m shape torch.Size([512]) 391\n",
      "7192\n",
      "[392, 1124]\n",
      "m shape torch.Size([512]) 392\n",
      "7192\n",
      "[393, 1124]\n",
      "m shape torch.Size([512]) 393\n",
      "7192\n",
      "[394, 9843]\n",
      "m shape torch.Size([512]) 394\n",
      "7192\n",
      "[395, 9374]\n",
      "m shape torch.Size([512]) 395\n",
      "7192\n",
      "[396, 9231]\n",
      "m shape torch.Size([512]) 396\n",
      "7192\n",
      "[397, 9320]\n",
      "m shape torch.Size([512]) 397\n",
      "7192\n",
      "[398, 8630]\n",
      "m shape torch.Size([512]) 398\n",
      "7192\n",
      "[399, 8514]\n",
      "m shape torch.Size([512]) 399\n",
      "7193\n",
      "[400, 8654]\n",
      "m shape torch.Size([512]) 400\n",
      "7193\n",
      "[401, 8025]\n",
      "m shape torch.Size([512]) 401\n",
      "7193\n",
      "[402, 6905]\n",
      "m shape torch.Size([512]) 402\n",
      "7192\n",
      "[403, 5352]\n",
      "m shape torch.Size([512]) 403\n",
      "7192\n",
      "[404, 4167]\n",
      "m shape torch.Size([512]) 404\n",
      "7192\n",
      "[411, 858]\n",
      "m shape torch.Size([512]) 411\n",
      "843\n",
      "[413, 539]\n",
      "m shape torch.Size([512]) 413\n",
      "842\n",
      "[415, 627]\n",
      "m shape torch.Size([512]) 415\n",
      "842\n",
      "[416, 701]\n",
      "m shape torch.Size([512]) 416\n",
      "842\n",
      "[417, 752]\n",
      "m shape torch.Size([512]) 417\n",
      "843\n",
      "[420, 517]\n",
      "m shape torch.Size([512]) 420\n",
      "533\n",
      "[422, 583]\n",
      "m shape torch.Size([512]) 422\n",
      "1073\n",
      "[423, 535]\n",
      "m shape torch.Size([512]) 423\n",
      "1073\n",
      "[424, 535]\n",
      "m shape torch.Size([512]) 424\n",
      "1073\n",
      "[425, 647]\n",
      "m shape torch.Size([512]) 425\n",
      "1073\n",
      "[426, 696]\n",
      "m shape torch.Size([512]) 426\n",
      "1073\n",
      "[432, 653]\n",
      "m shape torch.Size([512]) 432\n",
      "739\n",
      "[434, 873]\n",
      "m shape torch.Size([512]) 434\n",
      "961\n",
      "[435, 967]\n",
      "m shape torch.Size([512]) 435\n",
      "961\n",
      "[439, 731]\n",
      "m shape torch.Size([512]) 439\n",
      "684\n",
      "[445, 773]\n",
      "m shape torch.Size([512]) 445\n",
      "1403\n",
      "[447, 1085]\n",
      "m shape torch.Size([512]) 447\n",
      "1403\n",
      "[448, 1228]\n",
      "m shape torch.Size([512]) 448\n",
      "1404\n",
      "[454, 586]\n",
      "m shape torch.Size([512]) 454\n",
      "1128\n",
      "[455, 649]\n",
      "m shape torch.Size([512]) 455\n",
      "1129\n",
      "[456, 567]\n",
      "m shape torch.Size([512]) 456\n",
      "1129\n",
      "[457, 693]\n",
      "m shape torch.Size([512]) 457\n",
      "1128\n",
      "[458, 702]\n",
      "m shape torch.Size([512]) 458\n",
      "1128\n",
      "[459, 730]\n",
      "m shape torch.Size([512]) 459\n",
      "1128\n",
      "[460, 733]\n",
      "m shape torch.Size([512]) 460\n",
      "1128\n",
      "[461, 789]\n",
      "m shape torch.Size([512]) 461\n",
      "1129\n",
      "[462, 798]\n",
      "m shape torch.Size([512]) 462\n",
      "1128\n",
      "[463, 901]\n",
      "m shape torch.Size([512]) 463\n",
      "1128\n",
      "[464, 945]\n",
      "m shape torch.Size([512]) 464\n",
      "1128\n",
      "[465, 994]\n",
      "m shape torch.Size([512]) 465\n",
      "1128\n",
      "[466, 1024]\n",
      "m shape torch.Size([512]) 466\n",
      "1128\n",
      "[468, 1145]\n",
      "m shape torch.Size([512]) 468\n",
      "1128\n",
      "[469, 1049]\n",
      "m shape torch.Size([512]) 469\n",
      "1128\n",
      "[470, 1234]\n",
      "m shape torch.Size([512]) 470\n",
      "1128\n",
      "[472, 1430]\n",
      "m shape torch.Size([512]) 472\n",
      "1128\n",
      "[473, 554]\n",
      "m shape torch.Size([512]) 473\n",
      "1128\n",
      "[474, 799]\n",
      "m shape torch.Size([512]) 474\n",
      "1128\n",
      "[475, 909]\n",
      "m shape torch.Size([512]) 475\n",
      "1128\n",
      "[476, 942]\n",
      "m shape torch.Size([512]) 476\n",
      "1128\n",
      "[477, 1207]\n",
      "m shape torch.Size([512]) 477\n",
      "1128\n",
      "[478, 1118]\n",
      "m shape torch.Size([512]) 478\n",
      "1128\n",
      "[486, 893]\n",
      "m shape torch.Size([512]) 486\n",
      "2011\n",
      "[487, 976]\n",
      "m shape torch.Size([512]) 487\n",
      "2011\n",
      "[488, 1006]\n",
      "m shape torch.Size([512]) 488\n",
      "2011\n",
      "[489, 1021]\n",
      "m shape torch.Size([512]) 489\n",
      "2011\n",
      "[490, 1391]\n",
      "m shape torch.Size([512]) 490\n",
      "2011\n",
      "[491, 1403]\n",
      "m shape torch.Size([512]) 491\n",
      "2011\n",
      "[497, 532]\n",
      "m shape torch.Size([512]) 497\n",
      "2011\n",
      "[498, 788]\n",
      "m shape torch.Size([512]) 498\n",
      "2011\n",
      "[499, 857]\n",
      "m shape torch.Size([512]) 499\n",
      "2011\n",
      "[500, 945]\n",
      "m shape torch.Size([512]) 500\n",
      "2011\n",
      "[501, 1007]\n",
      "m shape torch.Size([512]) 501\n",
      "2011\n",
      "[502, 1012]\n",
      "m shape torch.Size([512]) 502\n",
      "2011\n",
      "[503, 1196]\n",
      "m shape torch.Size([512]) 503\n",
      "2011\n",
      "[504, 1268]\n",
      "m shape torch.Size([512]) 504\n",
      "2011\n",
      "[505, 1833]\n",
      "m shape torch.Size([512]) 505\n",
      "2011\n",
      "[506, 1890]\n",
      "m shape torch.Size([512]) 506\n",
      "2011\n",
      "[507, 1945]\n",
      "m shape torch.Size([512]) 507\n",
      "2011\n",
      "[509, 1913]\n",
      "m shape torch.Size([512]) 509\n",
      "2012\n",
      "[511, 547]\n",
      "m shape torch.Size([512]) 511\n",
      "4971\n",
      "[512, 766]\n",
      "m shape torch.Size([512]) 512\n",
      "4971\n",
      "[513, 964]\n",
      "m shape torch.Size([512]) 513\n",
      "4972\n",
      "[514, 1015]\n",
      "m shape torch.Size([512]) 514\n",
      "4971\n",
      "[515, 1034]\n",
      "m shape torch.Size([512]) 515\n",
      "4971\n",
      "[516, 1121]\n",
      "m shape torch.Size([512]) 516\n",
      "4972\n",
      "[517, 1134]\n",
      "m shape torch.Size([512]) 517\n",
      "4971\n",
      "[518, 1272]\n",
      "m shape torch.Size([512]) 518\n",
      "4971\n",
      "[519, 1591]\n",
      "m shape torch.Size([512]) 519\n",
      "4971\n",
      "[520, 1837]\n",
      "m shape torch.Size([512]) 520\n",
      "4971\n",
      "[521, 1894]\n",
      "m shape torch.Size([512]) 521\n",
      "4971\n",
      "[522, 1918]\n",
      "m shape torch.Size([512]) 522\n",
      "4972\n",
      "[523, 1953]\n",
      "m shape torch.Size([512]) 523\n",
      "4971\n",
      "[524, 1996]\n",
      "m shape torch.Size([512]) 524\n",
      "4972\n",
      "[525, 2013]\n",
      "m shape torch.Size([512]) 525\n",
      "4971\n",
      "[526, 2045]\n",
      "m shape torch.Size([512]) 526\n",
      "4972\n",
      "[527, 2142]\n",
      "m shape torch.Size([512]) 527\n",
      "4971\n",
      "[528, 2263]\n",
      "m shape torch.Size([512]) 528\n",
      "4971\n",
      "[529, 2315]\n",
      "m shape torch.Size([512]) 529\n",
      "4972\n",
      "[530, 2432]\n",
      "m shape torch.Size([512]) 530\n",
      "4972\n",
      "[531, 2488]\n",
      "m shape torch.Size([512]) 531\n",
      "4972\n",
      "[532, 2530]\n",
      "m shape torch.Size([512]) 532\n",
      "4972\n",
      "[533, 2530]\n",
      "m shape torch.Size([512]) 533\n",
      "4972\n",
      "[534, 2540]\n",
      "m shape torch.Size([512]) 534\n",
      "4972\n",
      "[535, 2565]\n",
      "m shape torch.Size([512]) 535\n",
      "4972\n",
      "[536, 2584]\n",
      "m shape torch.Size([512]) 536\n",
      "4972\n",
      "[537, 2643]\n",
      "m shape torch.Size([512]) 537\n",
      "4972\n",
      "[538, 2753]\n",
      "m shape torch.Size([512]) 538\n",
      "4972\n",
      "[539, 2816]\n",
      "m shape torch.Size([512]) 539\n",
      "4971\n",
      "[540, 2905]\n",
      "m shape torch.Size([512]) 540\n",
      "4971\n",
      "[541, 2922]\n",
      "m shape torch.Size([512]) 541\n",
      "4972\n",
      "[542, 2981]\n",
      "m shape torch.Size([512]) 542\n",
      "4972\n",
      "[543, 3042]\n",
      "m shape torch.Size([512]) 543\n",
      "4972\n",
      "[544, 3147]\n",
      "m shape torch.Size([512]) 544\n",
      "4971\n",
      "[545, 3433]\n",
      "m shape torch.Size([512]) 545\n",
      "4972\n",
      "[546, 3500]\n",
      "m shape torch.Size([512]) 546\n",
      "4972\n",
      "[547, 3572]\n",
      "m shape torch.Size([512]) 547\n",
      "4972\n",
      "[548, 3708]\n",
      "m shape torch.Size([512]) 548\n",
      "4972\n",
      "[549, 3814]\n",
      "m shape torch.Size([512]) 549\n",
      "4971\n",
      "[550, 3889]\n",
      "m shape torch.Size([512]) 550\n",
      "4972\n",
      "[551, 4374]\n",
      "m shape torch.Size([512]) 551\n",
      "4972\n",
      "[552, 4483]\n",
      "m shape torch.Size([512]) 552\n",
      "4971\n",
      "[553, 4528]\n",
      "m shape torch.Size([512]) 553\n",
      "4971\n",
      "[554, 4558]\n",
      "m shape torch.Size([512]) 554\n",
      "4972\n",
      "[555, 5159]\n",
      "m shape torch.Size([512]) 555\n",
      "4972\n",
      "[556, 5218]\n",
      "m shape torch.Size([512]) 556\n",
      "4972\n",
      "[557, 5326]\n",
      "m shape torch.Size([512]) 557\n",
      "4972\n",
      "[558, 5407]\n",
      "m shape torch.Size([512]) 558\n",
      "4971\n",
      "[559, 5435]\n",
      "m shape torch.Size([512]) 559\n",
      "4972\n",
      "[560, 6004]\n",
      "m shape torch.Size([512]) 560\n",
      "4971\n",
      "[561, 6027]\n",
      "m shape torch.Size([512]) 561\n",
      "4971\n",
      "[562, 6048]\n",
      "m shape torch.Size([512]) 562\n",
      "4971\n",
      "[564, 1062]\n",
      "m shape torch.Size([512]) 564\n",
      "1379\n",
      "[565, 1086]\n",
      "m shape torch.Size([512]) 565\n",
      "1380\n",
      "[566, 1073]\n",
      "m shape torch.Size([512]) 566\n",
      "1379\n",
      "[569, 916]\n",
      "m shape torch.Size([512]) 569\n",
      "1380\n",
      "[570, 916]\n",
      "m shape torch.Size([512]) 570\n",
      "1380\n",
      "[572, 926]\n",
      "m shape torch.Size([512]) 572\n",
      "1379\n",
      "[573, 1038]\n",
      "m shape torch.Size([512]) 573\n",
      "1379\n",
      "[574, 1055]\n",
      "m shape torch.Size([512]) 574\n",
      "1379\n",
      "[575, 1225]\n",
      "m shape torch.Size([512]) 575\n",
      "1379\n",
      "[577, 1086]\n",
      "m shape torch.Size([512]) 577\n",
      "1380\n",
      "[585, 709]\n",
      "m shape torch.Size([512]) 585\n",
      "850\n",
      "[586, 714]\n",
      "m shape torch.Size([512]) 586\n",
      "850\n",
      "[587, 732]\n",
      "m shape torch.Size([512]) 587\n",
      "850\n",
      "[609, 542]\n",
      "m shape torch.Size([512]) 609\n",
      "1886\n",
      "[610, 598]\n",
      "m shape torch.Size([512]) 610\n",
      "1886\n",
      "[611, 632]\n",
      "m shape torch.Size([512]) 611\n",
      "1886\n",
      "[612, 734]\n",
      "m shape torch.Size([512]) 612\n",
      "1887\n",
      "[613, 843]\n",
      "m shape torch.Size([512]) 613\n",
      "1886\n",
      "[614, 883]\n",
      "m shape torch.Size([512]) 614\n",
      "1886\n",
      "[615, 1007]\n",
      "m shape torch.Size([512]) 615\n",
      "1886\n",
      "[616, 1178]\n",
      "m shape torch.Size([512]) 616\n",
      "1886\n",
      "[617, 1495]\n",
      "m shape torch.Size([512]) 617\n",
      "1886\n",
      "[618, 1525]\n",
      "m shape torch.Size([512]) 618\n",
      "1886\n",
      "[619, 1624]\n",
      "m shape torch.Size([512]) 619\n",
      "1886\n",
      "[620, 1647]\n",
      "m shape torch.Size([512]) 620\n",
      "1887\n",
      "[621, 1652]\n",
      "m shape torch.Size([512]) 621\n",
      "1887\n",
      "[622, 1688]\n",
      "m shape torch.Size([512]) 622\n",
      "1886\n",
      "[623, 1954]\n",
      "m shape torch.Size([512]) 623\n",
      "1886\n",
      "[648, 693]\n",
      "m shape torch.Size([512]) 648\n",
      "1098\n",
      "[649, 701]\n",
      "m shape torch.Size([512]) 649\n",
      "1098\n",
      "[660, 589]\n",
      "m shape torch.Size([512]) 660\n",
      "1098\n",
      "[661, 644]\n",
      "m shape torch.Size([512]) 661\n",
      "1098\n",
      "[662, 1063]\n",
      "m shape torch.Size([512]) 662\n",
      "1098\n",
      "[663, 1088]\n",
      "m shape torch.Size([512]) 663\n",
      "1098\n",
      "[664, 1017]\n",
      "m shape torch.Size([512]) 664\n",
      "1098\n",
      "[676, 622]\n",
      "m shape torch.Size([512]) 676\n",
      "595\n",
      "[679, 525]\n",
      "m shape torch.Size([512]) 679\n",
      "616\n",
      "[680, 539]\n",
      "m shape torch.Size([512]) 680\n",
      "617\n",
      "[681, 543]\n",
      "m shape torch.Size([512]) 681\n",
      "616\n",
      "[682, 564]\n",
      "m shape torch.Size([512]) 682\n",
      "616\n",
      "[683, 589]\n",
      "m shape torch.Size([512]) 683\n",
      "617\n",
      "[684, 633]\n",
      "m shape torch.Size([512]) 684\n",
      "616\n",
      "[685, 635]\n",
      "m shape torch.Size([512]) 685\n",
      "617\n",
      "[689, 1404]\n",
      "m shape torch.Size([512]) 689\n",
      "5611\n",
      "[690, 1422]\n",
      "m shape torch.Size([512]) 690\n",
      "5611\n",
      "[691, 1428]\n",
      "m shape torch.Size([512]) 691\n",
      "5611\n",
      "[692, 1440]\n",
      "m shape torch.Size([512]) 692\n",
      "5611\n",
      "[693, 1455]\n",
      "m shape torch.Size([512]) 693\n",
      "5611\n",
      "[694, 1538]\n",
      "m shape torch.Size([512]) 694\n",
      "5611\n",
      "[695, 1541]\n",
      "m shape torch.Size([512]) 695\n",
      "5611\n",
      "[696, 1600]\n",
      "m shape torch.Size([512]) 696\n",
      "5611\n",
      "[697, 1606]\n",
      "m shape torch.Size([512]) 697\n",
      "5611\n",
      "[698, 1630]\n",
      "m shape torch.Size([512]) 698\n",
      "5612\n",
      "[699, 1652]\n",
      "m shape torch.Size([512]) 699\n",
      "5611\n",
      "[700, 1654]\n",
      "m shape torch.Size([512]) 700\n",
      "5611\n",
      "[701, 1673]\n",
      "m shape torch.Size([512]) 701\n",
      "5611\n",
      "[702, 1749]\n",
      "m shape torch.Size([512]) 702\n",
      "5611\n",
      "[703, 1791]\n",
      "m shape torch.Size([512]) 703\n",
      "5611\n",
      "[704, 2328]\n",
      "m shape torch.Size([512]) 704\n",
      "5611\n",
      "[705, 2681]\n",
      "m shape torch.Size([512]) 705\n",
      "5612\n",
      "[706, 2738]\n",
      "m shape torch.Size([512]) 706\n",
      "5611\n",
      "[707, 2837]\n",
      "m shape torch.Size([512]) 707\n",
      "5611\n",
      "[708, 2901]\n",
      "m shape torch.Size([512]) 708\n",
      "5611\n",
      "[709, 2903]\n",
      "m shape torch.Size([512]) 709\n",
      "5611\n",
      "[710, 2910]\n",
      "m shape torch.Size([512]) 710\n",
      "5611\n",
      "[711, 3087]\n",
      "m shape torch.Size([512]) 711\n",
      "5612\n",
      "[712, 3947]\n",
      "m shape torch.Size([512]) 712\n",
      "5611\n",
      "[713, 4175]\n",
      "m shape torch.Size([512]) 713\n",
      "5611\n",
      "[714, 5622]\n",
      "m shape torch.Size([512]) 714\n",
      "5611\n",
      "[715, 5965]\n",
      "m shape torch.Size([512]) 715\n",
      "5612\n",
      "[716, 5987]\n",
      "m shape torch.Size([512]) 716\n",
      "5612\n",
      "[717, 6020]\n",
      "m shape torch.Size([512]) 717\n",
      "5611\n",
      "[718, 6466]\n",
      "m shape torch.Size([512]) 718\n",
      "5611\n",
      "[719, 6895]\n",
      "m shape torch.Size([512]) 719\n",
      "5611\n",
      "[720, 6896]\n",
      "m shape torch.Size([512]) 720\n",
      "5611\n",
      "[721, 7017]\n",
      "m shape torch.Size([512]) 721\n",
      "5611\n",
      "[722, 7017]\n",
      "m shape torch.Size([512]) 722\n",
      "5611\n",
      "[723, 7065]\n",
      "m shape torch.Size([512]) 723\n",
      "5611\n",
      "[724, 7071]\n",
      "m shape torch.Size([512]) 724\n",
      "5611\n",
      "[725, 7071]\n",
      "m shape torch.Size([512]) 725\n",
      "5611\n",
      "[738, 626]\n",
      "m shape torch.Size([512]) 738\n",
      "682\n",
      "[739, 627]\n",
      "m shape torch.Size([512]) 739\n",
      "682\n",
      "[745, 719]\n",
      "m shape torch.Size([512]) 745\n",
      "7633\n",
      "[746, 763]\n",
      "m shape torch.Size([512]) 746\n",
      "7633\n",
      "[747, 799]\n",
      "m shape torch.Size([512]) 747\n",
      "7633\n",
      "[763, 529]\n",
      "m shape torch.Size([512]) 763\n",
      "646\n",
      "[764, 529]\n",
      "m shape torch.Size([512]) 764\n",
      "646\n",
      "[765, 579]\n",
      "m shape torch.Size([512]) 765\n",
      "647\n",
      "[766, 579]\n",
      "m shape torch.Size([512]) 766\n",
      "647\n",
      "[767, 666]\n",
      "m shape torch.Size([512]) 767\n",
      "646\n",
      "[768, 606]\n",
      "m shape torch.Size([512]) 768\n",
      "646\n",
      "[782, 547]\n",
      "m shape torch.Size([512]) 782\n",
      "565\n",
      "[801, 526]\n",
      "m shape torch.Size([512]) 801\n",
      "564\n",
      "[804, 526]\n",
      "m shape torch.Size([512]) 804\n",
      "564\n",
      "[829, 727]\n",
      "m shape torch.Size([512]) 829\n",
      "849\n"
     ]
    }
   ],
   "source": [
    "overflow_idx=[]\n",
    "[x[1] for x in loc_mentions]\n",
    "for i,j in enumerate(loc_mentions):\n",
    "    if j[1]>=512:\n",
    "        overflow_idx.append([i, j[1] ])\n",
    "        \n",
    "        \n",
    "        \n",
    "idx = [x[0] for x in overflow_idx]\n",
    "\n",
    "# res=list(np.array(bert_sentences)[idx])\n",
    "# len(res)\n",
    "\n",
    "m_new  = []\n",
    "for i, j in enumerate(overflow_idx):\n",
    "    print(j )\n",
    "    bert_sent = bert_sentences[j[0] ]\n",
    "    \n",
    "    #m[j[0]] = m[j[0], j[1]-4096:j[1]] \n",
    "    print(\"m shape\",m[j[0],  j[1]-512+1:j[1]+1].shape,j[0] )\n",
    "    m_new.append(m[j[0],  j[1]-512+1:j[1]+1] )\n",
    "    print(len(bert_sent.split()))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7cf44b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_new_tensor = torch.stack(m_new) \n",
    " \n",
    "#torch.nn.functional.pad(m_new_tensor, pad = (0,1), mode='constant', value=1)\n",
    "m_new_tensor = F.pad(m_new_tensor,pad =(0, m.shape[1]-m_new_tensor.shape[1]), value=1) \n",
    "m[idx] = m_new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3473f4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([847, 10946])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "6329dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = m == parallel_bert_large_model.module.vals[0]\n",
    "p = m == parallel_bert_large_model.module.vals[1]\n",
    "\n",
    "v = (k.int() + p.int()).bool()\n",
    "nz_indexes = v.nonzero()[:, 1].reshape(m.shape[0], 2)\n",
    "\n",
    "q = torch.arange(m.shape[1])\n",
    "q = q.repeat(m.shape[0], 1)\n",
    "\n",
    "msk_0 = (nz_indexes[:, 0].repeat(m.shape[1], 1).transpose(0, 1)) <= q\n",
    "msk_1 = (nz_indexes[:, 1].repeat(m.shape[1], 1).transpose(0, 1)) >= q\n",
    "\n",
    "msk_0_ar = (nz_indexes[:, 0].repeat(m.shape[1], 1).transpose(0, 1)) < q\n",
    "msk_1_ar = (nz_indexes[:, 1].repeat(m.shape[1], 1).transpose(0, 1)) > q\n",
    "\n",
    "attention_mask_g = msk_0.int() * msk_1.int()\n",
    "\n",
    "input_ids = input_ids[:, :512]\n",
    "# attention_mask = attention_mask[:, :4096]\n",
    "#attention_mask[:, 0] = 2\n",
    "#attention_mask[attention_mask_g == 1] = 2\n",
    "attention_mask = attention_mask[:, :512]\n",
    "arg1 = msk_0_ar.int() * msk_1_ar.int()\n",
    "arg_new1 = msk_0.int() * msk_1.int()\n",
    "arg_new1 = arg_new1[:,:512]\n",
    "arg1 = arg1[:, :512]\n",
    "arg1 = arg1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7ed16bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([847, 512]), torch.Size([847, 512]), torch.Size([847, 512]))"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, attention_mask.shape, arg1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f0c77908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating : 100%|██████████| 6/6 [00:00<00:00, 36738.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 512])\n",
      "0\n",
      "torch.Size([150, 512])\n",
      "150\n",
      "torch.Size([150, 512])\n",
      "300\n",
      "torch.Size([150, 512])\n",
      "450\n",
      "torch.Size([150, 512])\n",
      "600\n",
      "torch.Size([97, 512])\n",
      "750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 150\n",
    "batch = [input_ids[x] for x in range(i, min(i + batch_size, len(bert_sentences)))]\n",
    "for i in tqdm(range(0, len(bert_sentences), batch_size), desc=\"generating \"):\n",
    "    print(input_ids[i:i+batch_size].shape)\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9d849692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating : 100%|██████████| 17/17 [00:12<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vectors = []\n",
    "batch_size = 50\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(bert_sentences), batch_size), desc=\"generating \"):\n",
    "        #men_vectors = parallel_coref_model(input_ids[i:i+batch_size], attention_mask[i:i+batch_size], arg1[i:i+batch_size]).detach().cpu().numpy()\n",
    "        men_vectors = parallel_bert_large_model(input_ids[i:i+batch_size], attention_mask[i:i+batch_size], arg1[i:i+batch_size]) \n",
    "        all_vectors.extend(men_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "37d671ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_iterator = zip(men_ids, all_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5872c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_large_eve_vec_map_longdoc = dict(zip_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9430083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0027, -0.7967, -1.0430,  ..., -0.8410,  0.2529,  0.0932],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_large_eve_vec_map_longdoc['VMIC0015MXG.000003'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "3943e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_large_eve_mention_map_longdoc_file = working_folder + '/cbert_large_evt_vectorn_map_longbertdoc.pkl'\n",
    "#pickle.dump(bert_large_eve_vec_map_longdoc, open(bert_large_eve_mention_map_longdoc_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "64d3aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_large_vector_map_evt_longbertdoc = pickle.load(open(bert_large_eve_mention_map_longdoc_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e35c39b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0027, -0.7967, -1.0430,  ..., -0.8410,  0.2529,  0.0932],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_large_vector_map_evt_longbertdoc['VMIC0015MXG.000003'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f5804f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarities, _ = get_mention_pair_cosinesimilarity(mention_pairs, all_mention_map,bert_large_vector_map_evt_longbertdoc, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3a4027b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = np.asarray(similarities)\n",
    "np.max(similarities), np.min(similarities), np.mean(similarities)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cdd127",
   "metadata": {},
   "source": [
    "# bert large coref results with mean = 0.39 as cosine sim threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "68fb7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCUB SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (844 / 847) 99.64%\tPrecision: (58.5333491877758 / 847) 6.91%\tF1: 12.92%\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "MUC SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (573 / 576) 99.47%\tPrecision: (573 / 844) 67.89%\tF1: 80.7%\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "# get indices\n",
    "mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "row, col = zip(*mention_ind_pairs)\n",
    "# print(row, col)\n",
    "\n",
    "# create similarity matrix from the similarities\n",
    "n = len(curr_mentions)\n",
    "similarities = [int(x) for x in similarities ] # to avoid ValueError: row, column, and data arrays must be 1-D\n",
    "similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "# clustering algorithm and mention cluster map\n",
    "clusters, labels = connected_components(similarity_matrix)\n",
    "system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "# generate system key file\n",
    "system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "# evaluate\n",
    "generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c631dd0",
   "metadata": {},
   "source": [
    "# bert large coref results with mean = 0.47 as cosine sim threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "28b44b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarities, _ = get_mention_pair_cosinesimilarity(mention_pairs, all_mention_map,bert_large_vector_map_evt_longbertdoc, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1a4cdac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCUB SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (834.646008403361 / 847) 98.54%\tPrecision: (69.1464681153328 / 847) 8.16%\tF1: 15.07%\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "MUC SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (569 / 576) 98.78%\tPrecision: (569 / 834) 68.22%\tF1: 80.7%\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "# get indices\n",
    "mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "row, col = zip(*mention_ind_pairs)\n",
    "# print(row, col)\n",
    "\n",
    "# create similarity matrix from the similarities\n",
    "n = len(curr_mentions)\n",
    "similarities = [int(x) for x in similarities ] # to avoid ValueError: row, column, and data arrays must be 1-D\n",
    "similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "# clustering algorithm and mention cluster map\n",
    "clusters, labels = connected_components(similarity_matrix)\n",
    "system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "# generate system key file\n",
    "system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "# evaluate\n",
    "generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef857d",
   "metadata": {},
   "source": [
    "# bert large coref results with mean = 0.6 as cosine sim threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "84a5c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarities, _ = get_mention_pair_cosinesimilarity(mention_pairs, all_mention_map,bert_large_vector_map_evt_longbertdoc, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a14b32b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCUB SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (617.269597590231 / 847) 72.87%\tPrecision: (214.491595655806 / 847) 25.32%\tF1: 37.58%\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "MUC SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (458 / 576) 79.51%\tPrecision: (458 / 708) 64.68%\tF1: 71.33%\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "# get indices\n",
    "mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "row, col = zip(*mention_ind_pairs)\n",
    "# print(row, col)\n",
    "\n",
    "# create similarity matrix from the similarities\n",
    "n = len(curr_mentions)\n",
    "similarities = [int(x) for x in similarities ] # to avoid ValueError: row, column, and data arrays must be 1-D\n",
    "similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "# clustering algorithm and mention cluster map\n",
    "clusters, labels = connected_components(similarity_matrix)\n",
    "system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "# generate system key file\n",
    "system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "# evaluate\n",
    "generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649281a",
   "metadata": {},
   "source": [
    "# bert large coref results with mean = 0.7 as cosine sim threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1ac2254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/s/chopin/d/proj/ramfis-aida/venv/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarities, _ = get_mention_pair_cosinesimilarity(mention_pairs, all_mention_map,bert_large_vector_map_evt_longbertdoc, relations, working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "74f1e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCUB SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (375.806594427243 / 847) 44.36%\tPrecision: (410.370821900879 / 847) 48.44%\tF1: 46.31%\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "MUC SCORE\n",
      "====== TOTALS =======\n",
      "Identification of Mentions: Recall: (847 / 847) 100%\tPrecision: (847 / 847) 100%\tF1: 100%\n",
      "--------------------------------------------------------------------------\n",
      "Coreference: Recall: (298 / 576) 51.73%\tPrecision: (298 / 542) 54.98%\tF1: 53.3%\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the similarities of the mention-pairs\n",
    "# similarities = get_mention_pair_similarity_lemma(mention_pairs, all_mention_map, relations, working_folder)\n",
    "\n",
    "# get indices\n",
    "mention_ind_pairs = [(curr_men_to_ind[mp[0]], curr_men_to_ind[mp[1]]) for mp in mention_pairs]\n",
    "row, col = zip(*mention_ind_pairs)\n",
    "# print(row, col)\n",
    "\n",
    "# create similarity matrix from the similarities\n",
    "n = len(curr_mentions)\n",
    "similarities = [int(x) for x in similarities ] # to avoid ValueError: row, column, and data arrays must be 1-D\n",
    "similarity_matrix = csr_matrix((similarities, (row, col)), shape=(n, n)).toarray()\n",
    "\n",
    "# clustering algorithm and mention cluster map\n",
    "clusters, labels = connected_components(similarity_matrix)\n",
    "system_mention_cluster_map = [(men, clus) for men, clus in zip(curr_mentions, labels)]\n",
    "\n",
    "# generate system key file\n",
    "system_key_file = working_folder + f'/{men_type}_system.keyfile'\n",
    "generate_key_file(system_mention_cluster_map, men_type, working_folder, system_key_file)\n",
    "\n",
    "# evaluate\n",
    "generate_results(gold_key_file, system_key_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187048b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf029e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
